{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrds\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# We use this everywhere\n",
    "bea_codes = pd.read_excel('Data/User inputs/NAICS2BEA.xlsx')\n",
    "bea_seg = pd.read_stata('Data/Temp/levelkey.dta')\n",
    "\n",
    "# Ind_short to indcode\n",
    "bea_seg.rename(columns = {'ind_short': 'indcode'}, inplace = True)\n",
    "\n",
    "# Set up WRDS connection\n",
    "db = wrds.Connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regulation index\n",
    "\n",
    "All I do here is that I merge industry codes to the regulation index, and compute mean and median by industry group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data_reg = pd.read_excel('Data/Raw inputs/regdata_by_3-digit_industry.xls')\n",
    "\n",
    "# Rename columns, drop some\n",
    "data_reg.rename(inplace = True, columns = {'industry': 'naics', \n",
    "                'Industry-relevant restriction count (Industry Regulation Index)': 'regindex'})\n",
    "data_reg.drop(columns = ['Industry-relevant words'], inplace = True)\n",
    "\n",
    "# Create log of reg\n",
    "data_reg['logreg'] = np.log(data_reg['regindex'])\n",
    "\n",
    "# Merge with  BEA codes\n",
    "data_reg = data_reg.merge(bea_codes, how = 'left')\n",
    "\n",
    "# Merge with BEA segments\n",
    "data_reg = data_reg.merge(bea_seg, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some agg variables\n",
    "data_reg = data_reg.groupby(['indcode', 'year'])['regindex', 'logreg']\\\n",
    "                   .agg(['mean', 'median'])\\\n",
    "                   .reset_index()\n",
    "\n",
    "data_reg.columns = [x[0] + x[1] for x in data_reg.columns.to_flat_index()]\n",
    "data_reg.rename(columns = {\n",
    "    'regindexmean': 'a1m_regindex',\n",
    "    'regindexmedian': 'a1med_regindex',\n",
    "    'logregmean': 'a1m_logreg',\n",
    "    'logregmedian': 'a1med_logreg',\n",
    "}, inplace = True)\n",
    "\n",
    "# Save data\n",
    "data_reg.to_stata('Data/Intermediate/regindex_out.dta', write_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spreads\n",
    "\n",
    "Here I do some industry code merging and light aggregating for the industry spread datasets by Glichrist and Zakraj≈°ek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data_spread = pd.read_csv('Data/Raw inputs/spr_naics3_q.csv', parse_dates = ['date'])\n",
    "\n",
    "# Some date stuff\n",
    "data_spread['year'] = data_spread['date'].dt.year\n",
    "data_spread['qtr'] = data_spread['date'].dt.quarter\n",
    "\n",
    "# Keep only 4th quarter\n",
    "data_spread.query('qtr == 4', inplace = True)\n",
    "\n",
    "# Reanme and drop columns\n",
    "data_spread.rename(columns = {'naics3': 'naics'}, inplace = True)\n",
    "data_spread.drop(columns = ['date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with bea codes\n",
    "data_spread = data_spread.merge(bea_codes, how = 'left')\n",
    "\n",
    "# Merge with bea segments\n",
    "data_spread = data_spread.merge(bea_seg, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more or less aggregate stuff\n",
    "data_spread.eval('nb_spavg = nbonds * spr_avg', inplace = True)\n",
    "\n",
    "data_spread = data_spread.pivot_table(index = ['indcode', 'year'], \n",
    "                                      values = ['nbonds', 'nb_spavg'],\n",
    "                                      aggfunc = np.sum).reset_index()\n",
    "\n",
    "data_spread.eval('a1m_spread =  nb_spavg / nbonds', inplace = True)\n",
    "\n",
    "# Keep stuff\n",
    "data_spread = data_spread.filter(items = ['year', 'indcode', 'a1m_spread'])\n",
    "\n",
    "# Save data\n",
    "data_spread.to_stata('Data/Intermediate/spread_data.dta', write_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDII\n",
    "\n",
    "Here I do some industry code merging and light aggregating for the PDII occupational licensing datasets by Kleiner-Krueger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data_pdii = pd.read_stata('Data/Raw inputs/PDII_RDD_Survey.dta',\n",
    "                          columns = ['baseid', 'q11', 'q11a', 'industry'])\n",
    "\n",
    "# Generate new variables\n",
    "data_pdii['q11ind'] = None\n",
    "data_pdii.loc[data_pdii['q11'] == '2: no', 'q11ind'] = 0\n",
    "data_pdii.loc[data_pdii['q11'] == '1: yes', 'q11ind'] = 1\n",
    "\n",
    "data_pdii['q11aind'] = None\n",
    "data_pdii.loc[data_pdii['q11a'] == '2: no', 'q11aind'] = 0\n",
    "data_pdii.loc[data_pdii['q11a'] == '1: yes', 'q11aind'] = 1\n",
    "\n",
    "# Generate 3 digit naics\n",
    "data_pdii['naics'] = data_pdii.industry.astype(str).str.slice(0, 3).astype(int)\n",
    "\n",
    "# Drop stuff\n",
    "data_pdii.drop(columns = ['industry', 'q11', 'q11a'], inplace = True)\n",
    "\n",
    "# Make vars numeric\n",
    "data_pdii = data_pdii.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with bea codes\n",
    "data_pdii = data_pdii.merge(bea_codes, how = 'left')\n",
    "\n",
    "# Merge with bea segments\n",
    "data_pdii = data_pdii.merge(bea_seg, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate across ind_code\n",
    "data_pdii = data_pdii.pivot_table(index = 'indcode', values = ['q11ind', 'q11aind'],\n",
    "                                  aggfunc = np.mean).reset_index()\n",
    "\n",
    "# Rename\n",
    "data_pdii.rename(columns = {'q11ind': 'a1m_licensed', 'q11aind': 'a1m_licreq'}, inplace = True)\n",
    "\n",
    "# Save data\n",
    "data_pdii.to_stata('Data/Intermediate/license_out.dta', write_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bushee\n",
    "\n",
    "This merges the Bushee dataset with Thomson Reuters 13F.\n",
    "Bushee dataset is the one from the replication files, while the TR 13F is pulled from WRDS, specifically the `tfn.s34` database. I select the following columns:\n",
    "- `year` as the year part of `rdate`\n",
    "- `mgrno`, `shares`, `cusip`, `mgrname`\n",
    "\n",
    "I filter the date so as to keep only the entries where the month part of `rdate` is 12.\n",
    "\n",
    "After the merging I also group together some big investment firms. Here I exclude the recoding done in the stat file for Dimensional, because in the fund they assigned it the same number as Blackrock (typo I guess), and because all those firms they wanted to group under Dimensional already have the same number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data, replace none\n",
    "data_bushee = pd.read_stata('Data/Raw inputs/bushee_data_2015.dta')\n",
    "data_bushee = data_bushee.replace('.', np.nan)\n",
    "\n",
    "# Drop duplicated mgrno, year, keep max mgrno_v\n",
    "data_bushee.sort_values('mgrno_v', ascending= False).reset_index(inplace = True)\n",
    "data_bushee.drop_duplicates(['mgrno', 'year'], keep = 'last', inplace = True)\n",
    "\n",
    "# Rename, drop\n",
    "data_bushee = data_bushee.filter(items = ['year', 'mgrno', 'invpermclass'])\\\n",
    "                         .rename(columns = {'invpermclass': 'invclass'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Thompson Reuters 13F\n",
    "select_str = '''\n",
    "SELECT date_part('year', rdate) AS year, mgrno, shares, cusip, mgrname, no, sole, shared\n",
    "FROM tfn.s34 \n",
    "WHERE date_part('month', rdate) = 12\n",
    "'''\n",
    "\n",
    "data_13f = db.raw_sql(select_str)\n",
    "\n",
    "# Drop none cusip\n",
    "data_13f.dropna(subset = ['cusip'], inplace = True)\n",
    "\n",
    "# Drop duplicated mgrno&cusip&year -- in 97% of the cases shares are the same, \n",
    "# so it doesn't matter. As for the rest - if they don't care, neither do I\n",
    "data_13f.drop_duplicates(subset = ['mgrno', 'cusip', 'year'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Bushee with TR 13F\n",
    "data_13f = data_13f.merge(data_bushee, how = 'outer')\n",
    "\n",
    "# Replace invclass NAs with literal NA\n",
    "data_13f.loc[data_13f.invclass.isna(), 'invclass'] = 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variable for mgrno\n",
    "data_13f['mgrno_mapped'] = data_13f['mgrno']\n",
    "\n",
    "# Do some recoding\n",
    "data_13f.loc[data_13f.mgrname.str.contains('BLACKROCK') == True, 'mgrno_mapped'] = 11386\n",
    "data_13f.loc[data_13f.mgrname.str.contains('CAPITAL RESEARCH') == True, 'mgrno_mapped'] = 12740\n",
    "data_13f.loc[data_13f.mgrname.str.contains('VANGUARD GROUP') == True, 'mgrno_mapped'] = 90457\n",
    "\n",
    "fidelity = [\"FIDELITY INTERNATIONAL\", \"FIDELITY INTERNATL LTD\", \n",
    "    \"FIDELITY INTL LTD\", \"FIDELITY INTL. LTD.\", \"FIDELITY MANAGEMENT & RESEARCH\",\n",
    "    \"FIDELITY MGMT & RES CORP\", \"FIDELITY MGMT & RESEARCH (US)\", \"FIDELITY MGMT & RESEARCH CO\"]\n",
    "\n",
    "data_13f.loc[data_13f.mgrname.isin(fidelity), 'mgrno_mapped'] = 27700\n",
    "\n",
    "state_str = [\"STATE STR BK & TRUST CO BOSTON\", \"STATE STR CORP\", \n",
    "    \"STATE STR CORPORATION\", \"STATE STR GBL ADVR IRELAND LTD\", \"STATE STR RESEARCH & MGMT CO\",\n",
    "    \"STATE STR RESEARCH & MGMT CO.\", \"STATE STR RESR & MGMT\", \"STATE STREET BOSTON CORP\",\n",
    "    \"STATE STREET CORP\", \"STATE STREET RES. & MGMT\", \"STATE STREET RESR & MGMT\"]\n",
    "\n",
    "data_13f.loc[data_13f.mgrname.isin(state_str), 'mgrno_mapped'] = 81540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save what we have up to this point\n",
    "data_13f.to_stata('Data/Intermediate/bushee_detailed.dta', write_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage ownership\n",
    "\n",
    "Here I compute percentage ownership of each institution in each firm. For that I merge the existing Bushee/TF 13F database with shares outstanding CRSP database.\n",
    "\n",
    "That database is availible on WRDS as `crspa.msf`. I extract the following variables:\n",
    "- `permno, permco, cusip8, year, mkt, shrout`\n",
    "\n",
    "Here `cusip8` is the 8 character substring (starting from 1) of `cusip`, `year` is just the year part of `date` and `mkt` is the product of `prc` and `shrout`. I also filter to only entries with `month = 12`.\n",
    "\n",
    "After that I add the `gvkey` using the linking table provided by the authors. Finally, I compute the shares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some cols\n",
    "data_13f = data_13f.filter(items = ['cusip', 'year', 'invclass', 'shares'])\n",
    "\n",
    "# Sum by cusip, year, invclass\n",
    "data_13f = data_13f.pivot_table(index = ['cusip', 'year', 'invclass'], \n",
    "                                values = 'shares', aggfunc = np.sum)\\\n",
    "                   .reset_index()\n",
    "\n",
    "# Long to wide table, for some reason\n",
    "data_13f = data_13f.pivot_table(index = ['cusip', 'year'], columns = ['invclass'])\n",
    "data_13f.columns = data_13f.columns.droplevel(0)\n",
    "data_13f = data_13f.reset_index().rename(columns = {'cusip': 'cusip8'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CRSP stock data\n",
    "select_str = '''\n",
    "SELECT date_part('year', date) AS year, permno, permco, shrout,\n",
    "       shrout * abs(prc) AS mkt, substr(cusip, 1, 8) AS cusip8\n",
    "FROM crspa.msf\n",
    "WHERE date_part('month', date) = 12\n",
    "'''\n",
    "\n",
    "data_crsp = db.raw_sql(select_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with CRSP\n",
    "data_own = data_13f.merge(data_crsp, how = 'inner')\n",
    "\n",
    "# Sort by permco, year and mktcap and keep only largest mktcap within permco, year\n",
    "data_own = data_own.sort_values(['permco', 'year', 'mkt']).reset_index(drop = True)\n",
    "data_own.drop_duplicates(subset = ['permco', 'year'], keep = 'last', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in linking table, rename\n",
    "link_table = pd.read_stata('Data/Raw inputs/linkingTable.dta')\n",
    "link_table = link_table.rename(columns = {'lpermco': 'permco'})\n",
    "\n",
    "# Replace NA in linkenddt with today's date (as good as any)\n",
    "link_table.loc[link_table['linkenddt'].isna(), 'linkenddt'] = pd.to_datetime('today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge link table, do some filtering\n",
    "data_own = data_own.merge(link_table, how = 'inner')\n",
    "data_own.query('linkdt.dt.year <= year & linkenddt.dt.year >= year', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by gvkey, year, mkt and keep only the largest mkt\n",
    "data_own = data_own.sort_values(['gvkey', 'year', 'mkt']).reset_index(drop = True)\n",
    "data_own.drop_duplicates(subset = ['gvkey', 'year'], keep = 'last', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, compute shares\n",
    "data_own['sharesinstowned'] = data_own[['DED', 'NA', 'QIX', 'TRA']].sum(axis = 1)\n",
    "data_own['pctinsown'] = data_own['sharesinstowned']/(1000*data_own['shrout'])\n",
    "data_own['pctinsown'] = data_own['pctinsown'].clip(upper = 1)\n",
    "#data_own.loc[data_own.pctinsown.isna() & (data_own.year <= 2013), 'pctinsown'] = 0\n",
    "\n",
    "for type_s in ['DED', 'NA', 'QIX', 'TRA']:\n",
    "    pct_tot = data_own[type_s]/(1000*data_own['shrout'])\n",
    "    pct_ins = data_own[type_s]/data_own['sharesinstowned']\n",
    "    \n",
    "    data_own[f'pctsharetot{type_s}'] = pct_tot\n",
    "    data_own[f'pctshareins{type_s}'] = pct_ins\n",
    "    \n",
    "    data_own[f'pctsharetot{type_s}'] = pct_tot.clip(upper = 1)\n",
    "    data_own.loc[pct_tot.isna() & (data_own.year <= 2013), f'pctsharetot{type_s}'] = 0\n",
    "\n",
    "    data_own[f'pctshareins{type_s}'] = pct_ins.clip(upper = 1)\n",
    "    data_own.loc[pct_ins.isna() & (data_own.year <= 2013), f'pctshareins{type_s}'] = 0\n",
    "    \n",
    "    data_own.loc[data_own.year == 1980, f'pctshareins{type_s}'] = None\n",
    "    data_own.loc[data_own.year == 1980, f'pctsharetot{type_s}'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only some columns\n",
    "r = re.compile(\"pct.*\")\n",
    "keep_cols = ['gvkey', 'year', 'cusip8'] + list(filter(r.match, data_own.columns))\n",
    "data_own = data_own.filter(items = keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "data_own.to_stata('Data/Intermediate/bushee_firmmap.dta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
