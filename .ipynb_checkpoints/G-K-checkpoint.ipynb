{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# We use this everywhere\n",
    "bea_codes = pd.read_excel('Data/User inputs/NAICS2BEA.xlsx')\n",
    "bea_seg = pd.read_stata('Data/Temp/levelkey.dta')\n",
    "\n",
    "# Ind_short to indcode\n",
    "bea_seg.rename(columns = {'ind_short': 'indcode'}, inplace = True)\n",
    "\n",
    "# Set up WRDS connection\n",
    "db = wrds.Connection(wrds_username='tadej')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regulation index\n",
    "\n",
    "All I do here is that I merge industry codes to the regulation index, and compute mean and median by industry group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data_reg = pd.read_excel('Data/Raw inputs/regdata_by_3-digit_industry.xls')\n",
    "\n",
    "# Rename columns, drop some\n",
    "data_reg.rename(inplace = True, columns = {'industry': 'naics', \n",
    "                'Industry-relevant restriction count (Industry Regulation Index)': 'regindex'})\n",
    "data_reg.drop(columns = ['Industry-relevant words'], inplace = True)\n",
    "\n",
    "# Create log of reg\n",
    "data_reg['logreg'] = np.log(data_reg['regindex'])\n",
    "\n",
    "# Merge with  BEA codes\n",
    "data_reg = data_reg.merge(bea_codes, how = 'left')\n",
    "\n",
    "# Merge with BEA segments\n",
    "data_reg = data_reg.merge(bea_seg, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some agg variables\n",
    "gby = data_reg.groupby(['indcode', 'year'])\n",
    "\n",
    "data_reg['a1m_regindex'] = gby['regindex'].transform(np.mean)\n",
    "data_reg['a1med_regindex'] = gby['regindex'].transform(np.median)\n",
    "\n",
    "data_reg['a1m_logreg'] = gby['logreg'].transform(np.mean)\n",
    "data_reg['a1med_logreg'] = gby['logreg'].transform(np.median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spreads\n",
    "\n",
    "Here I do some industry code merging and light aggregating for the industry spread datasets by Glichrist and Zakraj≈°ek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data_spread = pd.read_csv('Data/Raw inputs/spr_naics3_q.csv', parse_dates = ['date'])\n",
    "\n",
    "# Some date stuff\n",
    "data_spread['year'] = data_spread['date'].dt.year\n",
    "data_spread['qtr'] = data_spread['date'].dt.quarter\n",
    "\n",
    "# Keep only 4th quarter\n",
    "data_spread.query('qtr == 4', inplace = True)\n",
    "\n",
    "# Reanme and drop columns\n",
    "data_spread.rename(columns = {'naics3': 'naics'}, inplace = True)\n",
    "data_spread.drop(columns = ['date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with bea codes\n",
    "data_spread = data_spread.merge(bea_codes, how = 'left')\n",
    "\n",
    "# Merge with bea segments\n",
    "data_spread = data_spread.merge(bea_seg, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more or less aggregate stuff\n",
    "data_spread.eval('nb_spavg = nbonds * spr_avg', inplace = True)\n",
    "\n",
    "data_spread = data_spread.pivot_table(index = ['indcode', 'year'], \n",
    "                                      values = ['nbonds', 'nb_spavg'],\n",
    "                                      aggfunc = np.sum).reset_index()\n",
    "\n",
    "data_spread.eval('a1m_spread =  nb_spavg / nbonds', inplace = True)\n",
    "\n",
    "# Keep stuff\n",
    "data_spread = data_spread.filter(items = ['year', 'indcode', 'a1m_spread'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDII\n",
    "\n",
    "Here I do some industry code merging and light aggregating for the PDII occupational licensing datasets by Kleiner-Krueger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data_pdii = pd.read_stata('Data/Raw inputs/PDII_RDD_Survey.dta',\n",
    "                          columns = ['baseid', 'q11', 'q11a', 'industry'])\n",
    "\n",
    "# Generate new variables\n",
    "data_pdii['q11ind'] = None\n",
    "data_pdii.loc[data_pdii['q11'] == '2: no', 'q11ind'] = 0\n",
    "data_pdii.loc[data_pdii['q11'] == '1: yes', 'q11ind'] = 1\n",
    "\n",
    "data_pdii['q11aind'] = None\n",
    "data_pdii.loc[data_pdii['q11a'] == '2: no', 'q11aind'] = 0\n",
    "data_pdii.loc[data_pdii['q11a'] == '1: yes', 'q11aind'] = 1\n",
    "\n",
    "# Generate 3 digit naics\n",
    "data_pdii['naics'] = data_pdii.industry.astype(str).str.slice(0, 3).astype(int)\n",
    "\n",
    "# Drop stuff\n",
    "data_pdii.drop(columns = ['industry', 'q11', 'q11a'], inplace = True)\n",
    "\n",
    "# Make vars numeric\n",
    "data_pdii = data_pdii.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with bea codes\n",
    "data_pdii = data_pdii.merge(bea_codes, how = 'left')\n",
    "\n",
    "# Merge with bea segments\n",
    "data_pdii = data_pdii.merge(bea_seg, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate across ind_code\n",
    "data_pdii = data_pdii.pivot_table(index = 'indcode', values = ['q11ind', 'q11aind'],\n",
    "                                  aggfunc = np.mean)\n",
    "\n",
    "# Rename\n",
    "data_pdii.rename(columns = {'q11ind': 'a1m_licensed', 'q11aind': 'a1m_licreq'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bushee\n",
    "\n",
    "This merges the Bushee dataset with Thomson Reuters 13F.\n",
    "Bushee dataset is the one from the replication files, while the TR 13F is pulled from WRDS, specifically the `tfn.s34` database. I select the following columns:\n",
    "- `year` as the year part of `rdate`\n",
    "- `mgrno`, `shares`, `cusip`, `mgrname`\n",
    "\n",
    "I filter the date so as to keep only the entries where the month part of `rdate` is 12.\n",
    "\n",
    "After the merging I also group together some big investment firms. Here I exclude the recoding done in the stat file for Dimensional, because in the fund they assigned it the same number as Blackrock (typo I guess), and because all those firms they wanted to group under Dimensional already have the same number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data, replace none\n",
    "data_bushee = pd.read_stata('Data/Raw inputs/bushee_data_2015.dta')\n",
    "data_bushee = data_bushee.replace('.', np.nan)\n",
    "\n",
    "# Drop duplicated mgrno, year, keep max mgrno_v\n",
    "data_bushee.sort_values('mgrno_v', ascending= False).reset_index(inplace = True)\n",
    "data_bushee.drop_duplicates(['mgrno', 'year'], keep = 'last', inplace = True)\n",
    "\n",
    "# Rename, drop\n",
    "data_bushee = data_bushee.filter(items = ['year', 'mgrno', 'invpermclass'])\\\n",
    "                         .rename(columns = {'invpermclass': 'invclass'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Thompson Reuters 13F\n",
    "select_str = '''\n",
    "SELECT date_part('year', rdate) AS year, mgrno, shares, cusip, mgrname\n",
    "FROM tfn.s34 \n",
    "WHERE date_part('month', rdate) = 12\n",
    "'''\n",
    "\n",
    "data_13f = db.raw_sql(select_str)\n",
    "\n",
    "# Drop none cusip\n",
    "data_13f.dropna(subset = ['cusip'], inplace = True)\n",
    "\n",
    "# Drop duplicated mgrno&cusip&year -- in 97% of the cases shares are the same, \n",
    "# so it doesn't matter. As for the rest - if they don't care, neither do I\n",
    "data_13f.drop_duplicates(subset = ['mgrno', 'cusip', 'year'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Bushee with TR 13F\n",
    "data_13f = data_13f.merge(data_bushee, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variable for mgrno\n",
    "data_13f['mgrno_mapped'] = data_13f['mgrno']\n",
    "\n",
    "# Do some recoding\n",
    "data_13f.loc[data_13f.mgrname.str.contains('BLACKROCK') == True, 'mrgno_mapped'] = 11386\n",
    "data_13f.loc[data_13f.mgrname.str.contains('CAPITAL RESEARCH') == True, 'mrgno_mapped'] = 12740\n",
    "data_13f.loc[data_13f.mgrname.str.contains('VANGUARD GROUP') == True, 'mrgno_mapped'] = 90457\n",
    "\n",
    "fidelity = [\"FIDELITY INTERNATIONAL\", \"FIDELITY INTERNATL LTD\", \n",
    "    \"FIDELITY INTL LTD\", \"FIDELITY INTL. LTD.\", \"FIDELITY MANAGEMENT & RESEARCH\",\n",
    "    \"FIDELITY MGMT & RES CORP\", \"FIDELITY MGMT & RESEARCH (US)\", \"FIDELITY MGMT & RESEARCH CO\"]\n",
    "\n",
    "data_13f.loc[data_13f.mgrname.isin(fidelity), 'mrgno_mapped'] = 27700\n",
    "\n",
    "state_str = [\"STATE STR BK & TRUST CO BOSTON\", \"STATE STR CORP\", \n",
    "    \"STATE STR CORPORATION\", \"STATE STR GBL ADVR IRELAND LTD\", \"STATE STR RESEARCH & MGMT CO\",\n",
    "    \"STATE STR RESEARCH & MGMT CO.\", \"STATE STR RESR & MGMT\", \"STATE STREET BOSTON CORP\",\n",
    "    \"STATE STREET CORP\", \"STATE STREET RES. & MGMT\", \"STATE STREET RESR & MGMT\"]\n",
    "\n",
    "data_13f.loc[data_13f.mgrname.isin(state_str), 'mrgno_mapped'] = 81540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save what we have up to this point\n",
    "data_13f.to_stata('Data/Intermediate/bushee_detailed.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage ownership\n",
    "\n",
    "Here I compute percentage ownership of each institution in each firm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some cols\n",
    "data_13f = data_13f.filter(items = ['cusip', 'year', 'invclass', 'shares'])\n",
    "\n",
    "# Sum by cusip, year, invclass\n",
    "data_13f = data_13f.pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'mgrno', 'shares', 'cusip', 'mgrname', 'invclass',\n",
       "       'mrgno_mapped'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_13f.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
