{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bea_downloader import BEADownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = BEADownloader('3E7CB4CE-838D-4BB0-AE7C-9BACB931D976')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEA\n",
    "\n",
    "Here I get the required tables from BEA, and add some classification codes from the replication stata file.\n",
    "After that I will compute some variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed assets\n",
    "\n",
    "Here a combination of tables (see code bellow), and concatenate them, and transform them to a long level (to add stuff later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in stat dict\n",
    "bea_key = pd.read_stata('Data/Temp/levelkey_bea.dta')\n",
    "\n",
    "# Generate a list of tables to read and their corresponding variable names\n",
    "table_num = {\n",
    "    '301': 'kp_',\n",
    "    '302': 'kq_',\n",
    "    '304': 'depp_',\n",
    "    '305': 'depq_',\n",
    "    '307': 'ip_',\n",
    "    '308': 'iq_',\n",
    "    '309': 'agep_'\n",
    "}\n",
    "\n",
    "table_suff = {\n",
    "    'E': 'eq_',\n",
    "    'S': 'st_',\n",
    "    'I': 'ip_',\n",
    "    'ESI': 'all_',\n",
    "}\n",
    "\n",
    "table_names = {}\n",
    "for tnn, tnl in table_num.items():\n",
    "    for tsn, tsl in table_suff.items():\n",
    "        table_names[tnn + tsn] = 'a1_' + tnl + tsl + 'bea'\n",
    "        \n",
    "# Download all tables\n",
    "tables = []\n",
    "for table_name, variable_label in table_names.items():\n",
    "    table = bd.get_table('FAAt' + table_name, frequency = 'A', \n",
    "                         database = 'FixedAssets').reset_index()\n",
    "    \n",
    "    table['var_name'] = variable_label\n",
    "    tables.append(table)\n",
    "\n",
    "# Concatenate the tables \n",
    "data_fa = pd.concat(tables).reset_index(drop=True)\n",
    "\n",
    "# Create indcode and year, drop date, rename LineDescription\n",
    "data_fa['year'] = data_fa.date.dt.year\n",
    "data_fa['indcode'] = data_fa.SeriesCode.str.slice(3,7)\n",
    "data_fa.drop(columns = ['date'], inplace = True)\n",
    "data_fa.rename(columns = {'LineDescription': 'ind'}, inplace = True)\n",
    "\n",
    "# To long format\n",
    "data_fa = data_fa.pivot_table(index = ['year', 'ind', 'indcode'], \n",
    "                    columns = ['var_name'])\n",
    "data_fa.columns = data_fa.columns.droplevel(0)\n",
    "data_fa = data_fa.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value added\n",
    "\n",
    "Here I use a merge of two downloaded excel tables (because API is too wierd). Note that on the BEA site it says that the date pre 1997 is not yet compatible with data post 1997, the authors seem to have ignored that.\n",
    "\n",
    "The tables are downloaded from \n",
    "- GDP By Industry -> Historical 1947-1997 data -> Annual zip tables\n",
    "- GDP By Industry -> Begin using data -> Annual zip tables\n",
    "\n",
    "I do some reshaping to get data in a semi-long format: so industry and year on the x axis, and type of value added (GOS, wages, taxes) on the y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to tidy data\n",
    "def tidy_va_data(data):\n",
    "    data.rename(columns = {'Unnamed: 1': 'ind'}, inplace = True)\n",
    "\n",
    "    # Replace some values\n",
    "    data.loc[data.ind.str.contains('Compensation'), 'ind'] = 'Wages'\n",
    "    data.loc[data.ind.str.contains('Taxes'), 'ind'] = 'Taxes'\n",
    "    data.loc[data.ind.str.contains('operating'), 'ind'] = 'GOS'\n",
    "    data['ind'] = data.ind.str.strip()\n",
    "\n",
    "    # Create a new column, modify name column\n",
    "    data['type'] = data.loc[data.ind.isin(['Wages', 'Taxes', 'GOS']), 'ind']\n",
    "    data.loc[data['type'].notna(), 'ind'] = None\n",
    "    data['ind'] = data.ind.fillna(method = 'ffill')\n",
    "    data['type'] = data['type'].fillna('Total')\n",
    "\n",
    "    # Table to long, and then to semi-long\n",
    "    data = data.melt(id_vars = ['ind', 'type'], var_name = 'year')\n",
    "    data['value'] = pd.to_numeric(data.value, errors = 'coerce')\n",
    "\n",
    "    # Get data to billions of dollars (to match FA)\n",
    "    data['value'] = data['value']/1000\n",
    "    \n",
    "    data = data.pivot_table(index = ['ind', 'year'], columns = ['type'], \n",
    "                              values = 'value').reset_index()\n",
    "    \n",
    "    # Year to numeric\n",
    "    data['year'] = pd.to_numeric(data.year)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and tidy data\n",
    "data_va = pd.read_excel('Data/Raw inputs/ValueAdded.xlsx', sheet_name = 'Components',\n",
    "                        skiprows = 5, skipfooter = 17).set_index('Line')\n",
    "data_va_h = pd.read_excel('Data/Raw inputs/ValueAdded_H.xlsx', sheet_name = 'Components',\n",
    "                        skiprows = 5, skipfooter = 17).set_index('Line')\n",
    "\n",
    "data_va = tidy_va_data(data_va)\n",
    "data_va_h = tidy_va_data(data_va_h)\n",
    "\n",
    "# Put them together\n",
    "data_va = pd.concat([data_va, data_va_h])\n",
    "data_va = data_va.sort_values(['ind', 'year']).reset_index(drop = True)\n",
    "\n",
    "# Drop total, rename the rest\n",
    "data_va.drop(columns = 'Total', inplace = True)\n",
    "data_va.rename(columns = {'Taxes': 'a1_taxes_bea',\n",
    "                          'Wages': 'a1_wages_bea',\n",
    "                          'GOS': 'a1_gos_bea'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gross output\n",
    "\n",
    "The datasets come in the zipfiles from Value Added section. Analysis is simpler, there is only one value column, so no extensive reshaping needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to tidy data\n",
    "def tidy_go_data(data):\n",
    "    data.rename(columns = {'Unnamed: 1': 'ind'}, inplace = True)\n",
    "    data['ind'] = data.ind.str.strip()\n",
    "\n",
    "    # Table to long\n",
    "    data = data.melt(id_vars = ['ind'], var_name = 'year')\n",
    "    data['value'] = pd.to_numeric(data.value, errors = 'coerce')\n",
    "    \n",
    "    # Get data to billions of dollars (to match FA)\n",
    "    data['value'] = data['value']/1000\n",
    "    \n",
    "    data.rename(columns = {'value': 'a1_output_bea'}, inplace = True)\n",
    "    \n",
    "    # Year to numeric\n",
    "    data['year'] = pd.to_numeric(data.year)\n",
    "    \n",
    "    # Drop if year les or equal 1963\n",
    "    data = data.query('year > 1963')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and tidy data\n",
    "data_go = pd.read_excel('Data/Raw inputs/GrossOutput.xlsx', sheet_name = 'GO',\n",
    "                        skiprows = 5, skipfooter = 8).set_index('Line')\n",
    "data_go_h = pd.read_excel('Data/Raw inputs/GrossOutput_H.xlsx', sheet_name = 'GO',\n",
    "                        skiprows = 5, skipfooter = 9).set_index('Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_go = tidy_go_data(data_go)\n",
    "data_go_h = tidy_go_data(data_go_h)\n",
    "\n",
    "# Put them together\n",
    "data_go = pd.concat([data_go, data_go_h])\n",
    "data_go = data_go.sort_values(['ind', 'year']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data\n",
    "\n",
    "Here I merge the three datasets, and add some variables from a stata matching file provided by the authors.\n",
    "\n",
    "When merging the key with the datasets, I perform an inner merge, which automatically throws away those industries we don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matching file, fix names (excel...)\n",
    "bea_key = pd.read_stata('Data/Temp/levelkey_bea.dta')\n",
    "bea_key['ind'] = bea_key.ind.str.replace(r'\\\\\\d\\\\', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FA and VA data\n",
    "data = data_fa.merge(data_va, on = ['ind', 'year'], how = 'outer')\n",
    "\n",
    "# Merge GO data\n",
    "data = data.merge(data_go, on = ['ind', 'year'], how = 'outer')\n",
    "\n",
    "# Merge FA data with key\n",
    "data = bea_key.merge(data, on = 'ind', how = 'inner')\n",
    "\n",
    "# Extend indcode\n",
    "ind = data.query('~indcode.isna()').groupby(['ind', 'indcode']).size()\n",
    "ind = ind.reset_index().drop(columns = 0)\n",
    "\n",
    "data = data.drop(columns = ['indcode']).merge(ind, on = 'ind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizations and aggregations\n",
    "\n",
    "Here I create some pivoting/aggregation\n",
    "- I make FA quantity variables nominal, by multiplying them by the 2009 value of their price counterpart, and divide by 100 \n",
    "- After that, I create a pivot table, summing accross `ind_short` and year, I also keep the `siccode` and discard the other descriptives.\n",
    "- During the course of the aggregations, some nan variables were transformed to 0, reverse that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agg variables, normalized to 2009\n",
    "p_vars = []\n",
    "q_vars = []\n",
    "\n",
    "for i in ['k', 'i', 'dep']:\n",
    "    for j in ['all_', 'eq_', 'st_', 'ip_']:\n",
    "        p_vars.append('a1_' + i + 'p_' + j + 'bea')\n",
    "        q_vars.append('a1_' + i + 'q_' + j + 'bea')\n",
    "\n",
    "vp_2009 = data.query('year == 2009')[['indcode'] + p_vars].set_index('indcode')\n",
    "vp_2009.columns =  q_vars\n",
    "div_2009 = lambda x: x.multiply(vp_2009.loc[x.name,:]/100)\\\n",
    "                      .replace([np.inf], np.nan)\n",
    "\n",
    "data.loc[:, q_vars] = data.groupby('indcode')[q_vars].apply(div_2009)\n",
    "\n",
    "# Kick out some variables, create pivot table\n",
    "data.drop(columns = ['indcode', 'ind', 'keep_ind', 'beacode'], inplace = True)\n",
    "\n",
    "data = data.melt(id_vars = ['year', 'ind_short', 'siccode'])\\\n",
    "           .groupby(['year', 'ind_short', 'variable'])\\\n",
    "           .agg({'value': [np.sum], 'siccode': [np.mean]})\n",
    "\n",
    "data.columns = data.columns.droplevel(1)\n",
    "data = data.set_index('siccode', append = True).unstack(2)\n",
    "data.columns = data.columns.droplevel(0)\n",
    "data = data.reset_index()\n",
    "\n",
    "# Reverse nans\n",
    "fa_cols = list(table_names.values())\n",
    "va_cols = ['a1_taxes_bea', 'a1_wages_bea', 'a1_gos_bea']\n",
    "go_cols = ['a1_output_bea']\n",
    "\n",
    "fa_m = data_fa.year.min(); fa_M = data_fa.year.max()\n",
    "va_m = data_va.year.min(); va_M = data_va.year.max()\n",
    "go_m = data_go.year.min(); go_M = data_go.year.max()\n",
    "\n",
    "data.loc[~data.year.between(fa_m, fa_M), fa_cols] = None\n",
    "data.loc[~data.year.between(va_m, va_M), va_cols] = None\n",
    "data.loc[~data.year.between(go_m, go_M), go_cols] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New variables\n",
    "\n",
    "Here I create some new variables:\n",
    "\n",
    "### Industry level\n",
    "- `a1_os_bea`: operatirng surplus (GOS - depreciation)\n",
    "- Net investment, investment rates and depreciation rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this to work we need to order data\n",
    "data.sort_values(['ind_short', 'year'], inplace = True)\n",
    "\n",
    "# Operating surplus\n",
    "data['a1_os_bea'] = data['a1_gos_bea'] - data['a1_depp_all_bea']\n",
    "\n",
    "# Generate new var names\n",
    "for i in ['all', 'eq', 'st', 'ip']:\n",
    "    var_defs = f'''\n",
    "        a1_depk_{i}_bea = a1_depp_{i}_bea/a1_kp_{i}_bea.shift(1)\n",
    "        a1_nip_{i}_bea = a1_ip_{i}_bea - a1_depp_{i}_bea\n",
    "        a1_ik_{i}_bea = a1_ip_{i}_bea/a1_kp_{i}_bea.shift(1)\n",
    "        a1_nik_{i}_bea = a1_nip_{i}_bea/a1_kp_{i}_bea.shift(1)\n",
    "        a1_dkp_{i}_bea = a1_kp_{i}_bea - a1_kp_{i}_bea.shift(1)\n",
    "        \n",
    "        a1_iy_{i}_bea = a1_ip_{i}_bea/a1_os_bea\n",
    "        a1_niy_{i}_bea = a1_nip_{i}_bea/a1_os_bea\n",
    "        a1_nigy_{i}_bea = a1_ip_{i}_bea/a1_gos_bea\n",
    "        \n",
    "        a1_depkq_{i}_bea = a1_depq_{i}_bea/a1_kq_{i}_bea.shift(1)\n",
    "        a1_niq_{i}_bea = a1_iq_{i}_bea/a1_depq_{i}_bea.shift(1)\n",
    "        a1_nikq_{i}_bea = a1_niq_{i}_bea/a1_kq_{i}_bea.shift(1)\n",
    "        a1_dkq_{i}_bea = (a1_kq_{i}_bea - a1_kq_{i}_bea.shift(1))/a1_kq_{i}_bea.shift(1)\n",
    "    '''\n",
    "    \n",
    "    data = data.groupby('ind_short').apply(lambda x: x.eval(var_defs))\\\n",
    "                                    .reset_index(drop = True)\\\n",
    "                                    .replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Limit some variables\n",
    "    data[f'a1_nigy_{i}_bea'] = data[f'a1_nigy_{i}_bea'].clip(-0.2, 2)\n",
    "    data[f'a1_niy_{i}_bea'] = data[f'a1_nigy_{i}_bea'].clip(-0.2, 2).replace(2, np.nan)\n",
    "    data[f'a1_iy_{i}_bea'] = data[f'a1_nigy_{i}_bea'].clip(-0.2, 2)\n",
    "\n",
    "# Some other stuff\n",
    "var_defs = '''\n",
    "    a1_depk_exip_bea = (a1_depp_all_bea-a1_depp_ip_bea)/(a1_kp_all_bea.shift(1)-a1_kp_ip_bea.shift(1))\n",
    "\n",
    "    a1_nip_exip_bea = (a1_ip_all_bea-a1_ip_ip_bea) - (a1_depp_all_bea-a1_depp_ip_bea)\n",
    "    a1_ik_exip_bea = (a1_ip_all_bea-a1_ip_ip_bea) / (a1_kp_all_bea.shift(1)-a1_kp_ip_bea.shift(1))\n",
    "    a1_nik_exip_bea = a1_nip_exip_bea / (a1_kp_all_bea.shift(1)-a1_kp_ip_bea.shift(1))\n",
    "\n",
    "    a1_osk_bea = a1_os_bea / a1_kp_all_bea.shift(1)\n",
    "'''\n",
    "\n",
    "data = data.groupby('ind_short').apply(lambda x: x.eval(var_defs))\\\n",
    "                                .reset_index(drop = True)\\\n",
    "                                .replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate level\n",
    "\n",
    "Basically the same stuff as above, but on aggregate level (i.e. across all industries for each year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-86-64a6f6d1b357>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-86-64a6f6d1b357>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    data[]\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gb = data.groupby('year')\n",
    "\n",
    "for i in ['all', 'eq', 'st', 'ip']:\n",
    "    \n",
    "    data[f'a_kp_{i}_bea'] = gb[f'a1_kp_{i}_all_bea'].transform(np.sum)\n",
    "    data[f'a_ip_{i}_bea'] = gb[f'a1_ip_{i}_all_bea'].transform(np.sum)\n",
    "    data[f'a_depp_{i}_bea'] = gb[f'a1_depp_{i}_all_bea'].transform(np.sum)\n",
    "    \n",
    "    data[f'a_kq_{i}_bea'] = gb[f'a1_kq_{i}_all_bea'].transform(np.sum)\n",
    "    data[f'a_iq_{i}_bea'] = gb[f'a1_iq_{i}_all_bea'].transform(np.sum)\n",
    "    data[f'a_depq_{i}_bea'] = gb[f'a1_depq_{i}_all_bea'].transform(np.sum)\n",
    "    \n",
    "    var_defs = f'''\n",
    "        a_nip_{i}_bea = a_ip_{i}_bea - a_depp_{i}_bea\n",
    "        a_ik_{i}_bea = a_ip_{i}_bea/l.a_kp_{i}_bea\n",
    "        a_depk_{i}_bea = a_depp_{i}_bea/l.a_kp_{i}_bea\n",
    "        a_nik_{i}_bea = a_nip_{i}_bea/l.a_kp_{i}_bea    \n",
    "\n",
    "        a_niq_{i}_bea = a_iq_{i}_bea - a_depq_{i}_bea\n",
    "        a_ikq_{i}_bea = a_iq_{i}_bea/l.a_kq_{i}_bea\n",
    "        a_depkq_{i}_bea = a_depq_{i}_bea/l.a_kq_{i}_bea\n",
    "        a_nikq_{i}_bea = a_niq_{i}_bea/l.a_kq_{i}_bea\n",
    "    '''\n",
    "    \n",
    "    data = data.groupby('ind_short').apply(lambda x: x.eval(var_defs))\\\n",
    "                                    .reset_index(drop = True)\\\n",
    "                                    .replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Some other stuff\n",
    "var_defs = '''\n",
    "    a_depk_exip_bea = (a_depp_all_bea-a_depp_ip_bea)/(a_kp_all_bea.shift(1)-a_kp_ip_bea.shift(1))\n",
    "    a_nip_exip_bea = (a_ip_all_bea-a_ip_ip_bea) - (a_depp_all_bea-a_depp_ip_bea)\n",
    "    a_ik_exip_bea = (a_ip_all_bea-a_ip_ip_bea) / (a_kp_all_bea.shift(1)-a_kp_ip_bea.shift(1))\n",
    "    a_nik_exip_bea = a_nip_exip_bea / (a_kp_all_bea.shift(1)-a_kp_ip_bea.shift(1))\n",
    "'''\n",
    "\n",
    "data = data.groupby('ind_short').apply(lambda x: x.eval(var_defs))\\\n",
    "                                .reset_index(drop = True)\\\n",
    "                                .replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "gb = data.groupby('year')\n",
    "\n",
    "# Some more other stuff\n",
    "data['a_gos_bea'] = gb['a1_gos_bea'].transform(np.sum)\n",
    "data['a_os_bea'] = gb['a1_os_bea'].transform(np.sum)\n",
    "data['a_output_bea'] = gb['a1_output_bea'].transform(np.sum)\n",
    "\n",
    "var_defs = '''\n",
    "    a_osk_bea = a_os_bea/a_kp_all_bea.shift(1)\n",
    "    a_iy_bea = a_ip_all_bea/a_os_bea\n",
    "    a_niy_bea = a_nip_all_bea/a_os_bea\n",
    "    a_nigy_bea = a_nip_all_bea/a_gos_bea\n",
    "'''\n",
    "\n",
    "data = data.groupby('ind_short').apply(lambda x: x.eval(var_defs))\\\n",
    "                                .reset_index(drop = True)\\\n",
    "                                .replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14003.4    44\n",
       "17295.3    44\n",
       "719.4      44\n",
       "5742.1     44\n",
       "4586.2     44\n",
       "1669.6     44\n",
       "666.8      44\n",
       "10861.3    44\n",
       "2310.1     44\n",
       "4378.6     44\n",
       "1523.8     44\n",
       "8343.7     44\n",
       "611.0      44\n",
       "18353.2    44\n",
       "2033.2     44\n",
       "9713.4     44\n",
       "5152.6     44\n",
       "1036.4     44\n",
       "788.2      44\n",
       "5426.5     44\n",
       "16226.7    44\n",
       "12866.7    44\n",
       "3057.0     44\n",
       "4205.9     44\n",
       "6694.7     44\n",
       "3686.2     44\n",
       "9382.7     44\n",
       "11884.9    44\n",
       "1826.8     44\n",
       "7886.6     44\n",
       "1123.4     44\n",
       "1255.4     44\n",
       "867.4      44\n",
       "10042.9    44\n",
       "517.2      44\n",
       "483.6      44\n",
       "948.8      44\n",
       "15526.1    44\n",
       "13557.2    44\n",
       "569.9      44\n",
       "3788.0     44\n",
       "15011.4    44\n",
       "5550.3     44\n",
       "27923.9    44\n",
       "3469.7     44\n",
       "6335.8     44\n",
       "16981.0    44\n",
       "14341.9    44\n",
       "8923.1     44\n",
       "6000.1     44\n",
       "17663.1    44\n",
       "497.9      44\n",
       "4007.2     44\n",
       "7041.5     44\n",
       "537.1      44\n",
       "14406.7    44\n",
       "2667.1     44\n",
       "4880.0     44\n",
       "0.0        41\n",
       "Name: a1_kp_all_bea, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('year')['a1_kp_all_bea'].transform(np.sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
