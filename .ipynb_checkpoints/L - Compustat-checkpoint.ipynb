{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ff_category import FFCategory\n",
    "from utilities import *\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Initialize connection\n",
    "db = wrds.Connection(wrds_username='tadej')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Use SIC (check it it means the latest or what) and NAICS instead of H versions [DONE]\n",
    "- Check the goodwill thing (where only max is kept)\n",
    "- Figure out how stata treats missing vars for lags\n",
    "- Did they miscode bank dependence??\n",
    "- Median/mean with NA [DONE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compustat\n",
    "\n",
    "Here I get the required compustat data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Annual fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download\n",
    "\n",
    "I get the a lot of variables from `funda` table in the `compd` database (see the SQL call bellow).\n",
    "\n",
    "The only filtering I do is to throw out things without a year (happens sometimes), and apply the standard, criptic data selection that prevents duplicates.\n",
    "\n",
    "Additionally, I do the join with the `compd.company` table to get `sic` and `naics` (**thank you EJMR**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL select string\n",
    "select_str = '''\n",
    "SELECT f.gvkey, fyear AS year, \n",
    "       CAST(c.sic AS integer) AS sic, CAST(c.naics AS integer) AS naics,\n",
    "       f.fic, csho, prcc_f, at, lt, pstk, dltt, dlc, \n",
    "       act, ppegt, dvt, prstkc, oiadp, txt, xint, che, ppent, capx,\n",
    "       gdwl, sppe, siv, ivstch, ivaco, scf, wcapc, dlcch, chech, ibc,\n",
    "       xidoc, dpc, txdc, esubc, sppiv, fopo, fsrco, exre, dltis, dltr,\n",
    "       sstk, ivch, aqc, fuseo, recch, invch, apalch, txach, aoloch,\n",
    "       fiao, dv, xrd, \"do\", txdb, ib, dp, sale, emp, dd1, dd3, dd5, aldo,\n",
    "       intan, pifo, pi, fopt, ap, invt, rect, cogs, xsga, oibdp, ivaeq, ivao,\n",
    "       dlrsn\n",
    "FROM compd.funda AS f INNER JOIN compd.company AS c ON c.gvkey = f.gvkey\n",
    "WHERE indfmt='INDL' AND datafmt='STD' AND popsrc='D' AND consol='C'\n",
    "        AND fyear IS NOT NULL\n",
    "'''\n",
    "\n",
    "# Get data\n",
    "data_fa = db.raw_sql(select_str)\n",
    "\n",
    "# Still there are, for some reason, 4 duplicates: remove them\n",
    "data_fa.drop_duplicates(subset = ['year', 'gvkey'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New variables\n",
    "\n",
    "Here I create the following new variables:\n",
    "- `me`: Market value of equity, obtained by multiplying common shares outsdanding with their price \n",
    "- `be`: Book value of equity, equal to total assets minus liabilities minus equity (some measure of it)\n",
    "- `bliab`: Book value of liabilities: assets minus equity\n",
    "- `mv`: Market value: Market value of equity + assets - book equity\n",
    "- `blev`: Leverage: book value of libilities divided by assets\n",
    "- `q`: Tobin's q (market value of the firm divided by assets)\n",
    "- `q2`: Alternative Tobin's q\n",
    "- `paya`: Payouts over assets\n",
    "- `bba`: Buybacks over assets\n",
    "- `os`: Net operating surplus\n",
    "- `payos`: Payouts over operating surplus\n",
    "- `bbos`: Buybacks over operating surplus\n",
    "- `ca`: Cash holdings over assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fa.eval('''\n",
    "    me = csho*prcc_f \n",
    "    be = at - lt - pstk \n",
    "    bliab = at - be\n",
    "    blev = bliab/at\n",
    "    mv = me + at - be\n",
    "    q = mv/at\n",
    "    q2 = (me+dltt+dlc-act)/ppegt\n",
    "    paya = (dvt + prstkc)/ at\n",
    "    bba = prstkc / at\n",
    "    os = oiadp - txt - xint  \n",
    "    payos = (dvt + prstkc)/ os\n",
    "    bbos = prstkc/ os\n",
    "    ca = che / at''', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peters and Taylor Q\n",
    "\n",
    "Here I download the Peters and Taylor (improved) Q measure, that is the `total_q` table in the `totalq` database.\n",
    "\n",
    "I download the variables `gvkey`, `year` (`fyear`), `q_tot`, `k_int` and `k_int_offbs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL select string\n",
    "select_str = '''\n",
    "SELECT fyear AS year, gvkey, q_tot, k_int, k_int_offbs\n",
    "FROM totalq.total_q\n",
    "'''\n",
    "\n",
    "# Get data\n",
    "data_q = db.raw_sql(select_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratings data\n",
    "\n",
    "I download the data from Compustat, specifically, I use the Ratings table under North America daily. I then apply a recoding of the ratings, as shown below.\n",
    "\n",
    "**Warning**: That dataset is obsolete, it has not been updated since 2017. There is a newer credit ratings dataset (under Capital Q), however, based on my investigation, it has much less data than the old one (and some data it has is ambiguous), so it is not a suitable substitute here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_str = '''\n",
    "SELECT date_part('year', datadate) AS year, splticrm, gvkey\n",
    "FROM compd.adsprate\n",
    "WHERE date_part('month', datadate) = 12\n",
    "AND splticrm IS NOT NULL\n",
    "AND splticrm NOT IN ('N.M.', 'Suspended', 'SD')\n",
    "'''\n",
    "\n",
    "data_r = db.raw_sql(select_str)\n",
    "\n",
    "# Numerify the ratings\n",
    "replace_dict = {\n",
    "    'AAA': 1, 'AA': 2, 'A': 3, 'BBB': 4, 'BB': 5, 'B': 6,\n",
    "    'CCC': 7, 'CC': 8, 'C': 9, 'D': 10\n",
    "}\n",
    "\n",
    "data_r['sprating'] = data_r['splticrm']\n",
    "\n",
    "# The order is important here\n",
    "for key, value in replace_dict.items():\n",
    "    data_r.loc[data_r.sprating.str.contains(key), 'sprating'] = str(value)\n",
    "    \n",
    "data_r['sprating'] = data_r['sprating'].astype(int) \n",
    "\n",
    "# Create some rating booleans\n",
    "data_r['AAtoAAA'] = (data_r.sprating <= 2).astype(int)\n",
    "data_r['BBBtoA'] = ((data_r.sprating <= 4) & (data_r.sprating >= 3)).astype(int)\n",
    "\n",
    "# Drop some variables\n",
    "data_r.drop(columns = ['splticrm'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and minor calculations\n",
    "\n",
    "Here I merge the three datasets downloaded so far, and compute some new variables:\n",
    "- `k_pt`: A measure of capital, basically adding physical and intangible capital together\n",
    "- `shareintant`: Share of intangible capital in the above measure\n",
    "\n",
    "After that, I sort data by `gvkey` and `year` and add cummulative count for `gvkey` : this will be the age of the firm (assuming it appears in the dataset every year of its existence). I then log this to produce `logage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge fundamentals annual and Q data and then ratings as well\n",
    "data = data_fa.merge(data_q, on = ['gvkey', 'year'], how = 'left')\\\n",
    "              .merge(data_r, on = ['gvkey', 'year'], how = 'left')\n",
    "\n",
    "# New variables\n",
    "data.eval('''\n",
    "    k_pt = ppent + k_int\n",
    "    shareintan = k_int / k_pt''', inplace = True)\n",
    "\n",
    "# Sort and age\n",
    "data.sort_values(['gvkey', 'year'], inplace = True)\n",
    "data['age'] = data.groupby('gvkey').cumcount()\n",
    "data['logage'] = np.log(data.groupby('gvkey').cumcount() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregates pre-filtering\n",
    "\n",
    "Here I compute some aggregates (by year), which are later used to produce a graph. These aggregates are:\n",
    "- `a_capx_all_preEx`: sum of `capx`\n",
    "- `a_capx_US_preEx`: sum of `capx`, only for US firms\n",
    "- `a_pay_preEx`: sum of `dvt` and `prstkc`\n",
    "- `a_prstkc_preEx`: sum of `prstkc`\n",
    "- `a_at_preEx`: sum of `at`\n",
    "- `a_paya_preEx = a_pay_preEx / a_at_preEx`\n",
    "- `a_bba_preEx = a_prstkc_preEx / a_at_preEx`\n",
    "\n",
    "**Note**: Check the thing about goodwill later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute aggregate variables from data\n",
    "data_agg = data.groupby('year').apply(lambda x: pd.Series({\n",
    "    'a_capx_all_preEx': x['capx'].sum(min_count = 1),\n",
    "    'a_capx_US_preEx': x.query('fic == \"USA\"')['capx'].sum(min_count = 1),\n",
    "    'a_pay_preEx': x.eval('dvt + prstkc').sum(min_count = 1),\n",
    "    'a_prstkc_preEx': x['prstkc'].sum(min_count = 1),\n",
    "    'a_at_preEx': x['at'].sum(min_count = 1)\n",
    "}))\n",
    "\n",
    "# Add some composite aggregate variables\n",
    "data_agg.eval('''\n",
    "    a_paya_preEx = a_pay_preEx / a_at_preEx\n",
    "    a_bba_preEx = a_prstkc_preEx / a_at_preEx''', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and aggregates\n",
    "\n",
    "Finally, I do some filtering:\n",
    "1. Drop all entries where `at`, `gvkey`, `be`, `me`, `bliab` or `q` are NA, as well as those where `at` is smaller than 1 (million), and where either `be` or `me` are negative.\n",
    "2. Drop all entries with years smaller or equal to 1961, and drop a peculiar case with `gvkey` 4828 and `year` 2001.\n",
    "3. Drop all entries with `sic` in \\[4900, 4999\\] (utilities), \\[6000, 6999\\] (financials) or \\[5300, 5399\\] (real estate)\n",
    "4. Keep only US companies (`fic = USA`)\n",
    "\n",
    "At this point I also compute some aggregates (I do this right after step 3).\n",
    "\n",
    "Finally, I assign the Fama-French categories to the SIC numbers (based on my own implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs, filtering\n",
    "data.dropna(subset = ['year', 'gvkey', 'be', 'me', 'bliab', 'q'], inplace = True)\n",
    "data.query('at >= 1 & be > 0 & me > 0 & year > 1961 & ~(gvkey == 4828 & year == 2001)', inplace = True)\n",
    "\n",
    "# Exclude some sic codes\n",
    "data.query('~sic.between(4900, 4999) & ~sic.between(6000, 6999) & ~sic.between(5300, 5399)', inplace = True)\n",
    "\n",
    "# Some additional aggregates\n",
    "k = data.groupby('year').apply(lambda x: pd.Series({\n",
    "    'a_capx_all_wEx': x['capx'].sum(min_count = 1),\n",
    "    'a_capx_US_wEx': x.query('fic == \"USA\"')['capx'].sum(min_count = 1),\n",
    "}))\n",
    "\n",
    "data_agg[['a_capx_all_wEx', 'a_capx_US_wEx']] = k\n",
    "\n",
    "# Drop non-USA\n",
    "data.query('fic == \"USA\"', inplace = True)\n",
    "\n",
    "# Get the 10 category FF classification\n",
    "ff = FFCategory('https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/Siccodes10.zip')\n",
    "data['ff10'] = ff.assign_ff(data.sic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 10\n",
    "\n",
    "Here I reproduce the table 10 from the paper, which shows quantiles for different Q measures over two time periods: 1975 - 1980 and 2010 - 2015.\n",
    "\n",
    "As you can see in the table bellow (and by comparing it with the one from the paper), we have almost the exact same numbers, except for extreme quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">1975-1980</th>\n",
       "      <th colspan=\"3\" halign=\"left\">2010-2015</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>q</th>\n",
       "      <th>q2</th>\n",
       "      <th>q_tot</th>\n",
       "      <th>q</th>\n",
       "      <th>q2</th>\n",
       "      <th>q_tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-6.3</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>1.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>34.1</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>2.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>6.5</td>\n",
       "      <td>99.4</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>6.6</td>\n",
       "      <td>26.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>16.4</td>\n",
       "      <td>inf</td>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1975-1980             2010-2015            \n",
       "             q    q2 q_tot         q    q2 q_tot\n",
       "0.01       0.5  -4.8  -1.0       0.5  -6.3  -0.9\n",
       "0.05       0.7  -1.5  -0.5       0.7  -0.6  -0.2\n",
       "0.10       0.7  -0.8  -0.3       0.9   0.0   0.0\n",
       "0.25       0.8  -0.2  -0.1       1.1   0.7   0.3\n",
       "0.50       1.0   0.3   0.2       1.6   2.2   0.8\n",
       "0.75       1.3   0.9   0.6       2.5   8.1   1.6\n",
       "0.90       1.9   2.5   1.4       4.4  34.1   3.4\n",
       "0.95       2.7   4.7   2.4       6.5  99.4   5.9\n",
       "0.99       6.6  26.1   9.6      16.4   inf  23.6"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantiles to get\n",
    "quantiles = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "    \n",
    "# Get quantiles\n",
    "q1 = data.query('year.between(1975, 1980)')[['q', 'q2', 'q_tot']].quantile(quantiles).round(1)\n",
    "q2 = data.query('year.between(2010, 2015)')[['q', 'q2', 'q_tot']].quantile(quantiles).round(1)\n",
    "\n",
    "# Joint table\n",
    "qs_table = pd.concat([q1, q2], keys = ['1975-1980', '2010-2015'], axis = 1)\n",
    "\n",
    "# Save to tables\n",
    "qs_table.to_csv('Tables/Table10.csv')\n",
    "\n",
    "# Show results\n",
    "qs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation mapping\n",
    "\n",
    "Here I create some segmentation variables (mostly NAICS related)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAICS 3\n",
    "\n",
    "I create a new variable `naics3` from the first 3 digits of `naics`. Where this is missing, I do the following assignment:\n",
    "- For each sic and naics pair I count the number of entries appearing in it.\n",
    "- For each sic, I take the highest NAICS (from the previous step)\n",
    "- When possible, I match this naics to corresponding SIC\n",
    "\n",
    "Additionally, I also manually adjust some naics values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naics to string\n",
    "data['naics_str'] = data.naics.astype(str).str.slice(0, -2)\n",
    "data.loc[data.naics.isna(), 'naics_str'] = np.nan\n",
    "\n",
    "# Default NAICS3\n",
    "data['naics3'] = data.loc[data.naics_str.str.len() >= 3, 'naics_str'].str.slice(0,3)\n",
    "data['naics3'] = data['naics3'].astype(float)\n",
    "\n",
    "# Create the matching dictionary\n",
    "agg = data.groupby(['sic', 'naics3']).size()\n",
    "match_dict = agg.groupby(level = 0, group_keys = False).nlargest(1)\n",
    "match_dict = match_dict.reset_index().drop(columns= [0])\n",
    "\n",
    "# Reset index before merging\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Match\n",
    "naics3_matched = data[['sic']].merge(match_dict, on = 'sic', how = 'left')['naics3']\n",
    "data.loc[data.naics3.isna(), 'naics3'] = naics3_matched\n",
    "\n",
    "# Special matching\n",
    "data.loc[data.naics3.isna() & (data.sic == 3412), 'naics3'] = 332"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEA Codes\n",
    "\n",
    "Here I map to BEA codes, segments and industry, using the provided excel and stata files. After that I merge with the computed BEA file, using the industry depreciation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files\n",
    "bea_codes = pd.read_excel('Data/User inputs/NAICS2BEA.xlsx').rename(columns={'naics': 'naics3'})\n",
    "bea_segments = pd.read_stata('Data/Temp/levelkey.dta').rename(columns={'ind_short': 'indcode'})\n",
    "bea_industry = pd.read_stata('Data/Intermediate/BEA_industry.dta').rename(columns={'ind_short': 'indcode'})\n",
    "\n",
    "# Some filtering on bea_industry\n",
    "bea_industry = bea_industry.loc[:, ['indcode', 'year'] + [x for x in bea_industry.columns if 'a1_depk_' in x]]\n",
    "\n",
    "# Perform the merges (they have to be done in sequence)\n",
    "data = data.merge(bea_codes, how = 'left')\\\n",
    "           .merge(bea_segments, how = 'inner')\\\n",
    "           .merge(bea_industry, how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAICS 6 + 4\n",
    "\n",
    "I create a new variable `naics6` from the first 6 digits of `naics`. Same adjustment for missing as in the case of `naics3`.\n",
    "After this matching, I take the first 4 digits of `naics6` and create `naics4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 6 letter substring of naics\n",
    "data['naics6'] = data.naics_str.str.slice(0,6)\n",
    "data.loc[data['naics6'].str.len() < 6, 'naics6'] = None\n",
    "data['naics6'] = data['naics6'].astype(float)\n",
    "\n",
    "# Create the matching dictionary\n",
    "agg = data.groupby(['sic', 'naics6']).size()\n",
    "match_dict = agg.groupby(level = 0, group_keys = False).nlargest(1)\n",
    "match_dict = match_dict.reset_index().drop(columns= [0])\n",
    "\n",
    "# Reset index before merging\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Match\n",
    "naics6_matched = data[['sic']].merge(match_dict, on = 'sic', how = 'left')['naics6']\n",
    "data.loc[data.naics6.isna(), 'naics6'] = naics6_matched\n",
    "\n",
    "# Generate naics4\n",
    "data['naics4'] = data['naics6'].astype(str).replace('nan', np.nan).str.slice(0,4).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New variables\n",
    "\n",
    "Here I create a bunch of new variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financing needs and issuance\n",
    "\n",
    "Here I compute financing deficit and issuance following Frank and Goyal (2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that mimics Stata's rowtotal\n",
    "def rowtotal(data, cols, signs = None):\n",
    "    if signs == None:\n",
    "        signs = np.repeat(1, len(cols))\n",
    "    else:\n",
    "        signs = np.array(signs)\n",
    "\n",
    "    result = (data[cols] * signs).sum(axis = 1, min_count = 1)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some variables, for the additions on them we will want to treat NAs as 0\n",
    "invdef13 = rowtotal(data, ['capx', 'ivch', 'aqc', 'fuseo', 'sppe', 'siv'], [1,1,1,1,-1,-1])\n",
    "invdef7 = rowtotal(data, ['capx', 'ivch', 'aqc', 'sppe', 'siv', \n",
    "                                  'ivstch', 'ivaco'], [1,1,1,-1,-1,-1,-1])\n",
    "\n",
    "dnwc1 = rowtotal(data, ['wcapc', 'chech', 'dlcch'], [1,1,1])\n",
    "dnwc23 = rowtotal(data, ['wcapc', 'chech', 'dlcch'], [-1,1,-1])\n",
    "\n",
    "dnwc7 = -rowtotal(data, ['recch', 'invch', 'apalch', 'txach', \n",
    "                                 'aoloch', 'chech', 'fiao', 'dlcch'], [1,1,1,1,1,-1,1,1])\n",
    "incf13 = rowtotal(data, ['ibc', 'xidoc', 'dpc', 'txdc', 'esubc', 'sppiv', 'fopo', 'fsrco'])\n",
    "incf7 = rowtotal(data, ['ibc', 'xidoc', 'dpc', 'txdc', 'esubc', 'sppiv', 'fopo', 'exre'])\n",
    "\n",
    "# Compute some composite variables\n",
    "data['inv_def'] = np.select([data['scf'].between(1, 3), data['scf'] == 7],\n",
    "                            [invdef13, invdef7], np.nan)\n",
    "\n",
    "data['dnwc_def'] = np.select([data['scf'] == 1, data['scf'].between(2, 3), data['scf'] == 7],\n",
    "                             [dnwc1, dnwc23, dnwc7], np.nan)\n",
    "\n",
    "data['incf_def'] = np.select([data['scf'].between(1, 3), data['scf'] == 7],\n",
    "                             [incf13, incf7], np.nan)\n",
    "\n",
    "# Compute finance deficit and issuance\n",
    "data.eval('''\n",
    "    findef = dv + inv_def + dnwc_def - incf_def \n",
    "    ndebtiss = dltis - dltr\n",
    "    neqiss = sstk - prstkc\n",
    "''', inplace = True)\n",
    "\n",
    "# Some NA stuff\n",
    "new_vars = ['findef', 'ndebtiss', 'neqiss', 'inv_def', 'dnwc_def', 'incf_def']\n",
    "data[new_vars[:3]] = data[new_vars[:3]].dropna()\n",
    "data.loc[data['year'] < 1971, new_vars] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort first\n",
    "data = data.sort_values(['gvkey', 'year']).reset_index(drop = True)\n",
    "\n",
    "# Create some stuff with lags/leads\n",
    "data['shift_not_ok'] = data['gvkey'] != data['gvkey'].shift(1)\n",
    "data['at_l'] = data['at'].shift(1)\n",
    "data.loc[data['shift_not_ok'], 'at_l'] = None\n",
    "\n",
    "data.eval('''\n",
    "    cdat = dv/(at+at_l)\n",
    "    invdefat = inv_def/(at+at_l)\n",
    "    dwcat = dnwc_def/(at+at_l)\n",
    "    dincfat = incf_def/(at+at_l)\n",
    "    \n",
    "    defat = findef/(at+at_l)\n",
    "    diat = ndebtiss/(at+at_l)\n",
    "    eiat = neqiss/(at+at_l)\n",
    "    dfpct = diat/defat\n",
    "    efpct = eiat/defat\n",
    "''', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim some vars\n",
    "data['bba'] = data['bba'].clip(upper = 0.1)\n",
    "data['paya'] = data['paya'].clip(upper = 0.1)\n",
    "data['bbos'] = data['bbos'].clip(upper = 2)\n",
    "data['q'] = data['q'].clip(upper = 10)\n",
    "data['q2'] = data['q2'].clip(upper = 15)\n",
    "data['q_tot'] = data['q_tot'].clip(upper = 10)\n",
    "\n",
    "data['q_tot'] = data.groupby('year')['q_tot'].transform(winsor)\n",
    "data['defat'] = data.groupby('year')['defat'].transform(winsor)\n",
    "data['diat'] = data.groupby('year')['diat'].transform(winsor)\n",
    "data['eiat'] = data.groupby('year')['eiat'].transform(winsor)\n",
    "data['dfpct'] = data.groupby('year')['dfpct'].transform(winsor)\n",
    "data['efpct'] = data.groupby('year')['efpct'].transform(winsor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some aggregates\n",
    "var_agg = ['defat', 'diat', 'eiat', 'dfpct', 'efpct', 'cdat', 'invdefat', 'dwcat', 'dincfat']\n",
    "\n",
    "for var in var_agg:\n",
    "    data[f'a1m_{var}'] = data.groupby(['indcode', 'year'])[var].transform(np.mean)\n",
    "    data[f'am_{var}'] = data.groupby(['year'])[var].transform(np.mean)\n",
    "    data[f'a1med_{var}'] = data.groupby(['indcode', 'year'])[var].transform(np.median)\n",
    "    data[f'amed_{var}'] = data.groupby(['year'])[var].transform(np.median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core variables for investment analyses\n",
    "\n",
    "Here I compute the following definitions:\n",
    "1. Capx/PP&E\n",
    "2. dIntan/Intan --> from Peters & Taylor\n",
    "3. R&D/assets\n",
    "4. (Capx + R&D)/assets\n",
    "5. Net I/K \n",
    "6. dAT/AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust xrd\n",
    "data['xrd'] = data['xrd'].fillna(0)\n",
    "\n",
    "# Create a bunch of vars\n",
    "data.eval('''\n",
    "    inv1 = capx\n",
    "    kdef1 = ppent\n",
    "    inv2 = k_int.diff(1) \n",
    "    kdef2 = k_int\n",
    "    inv3 = xrd\n",
    "    kdef3 = at\n",
    "    inv4 = capx + xrd \n",
    "    kdef4 = at\n",
    "''', inplace = True)\n",
    "\n",
    "# Before we proceed, let's fix the diff and some NAs\n",
    "data.loc[data['shift_not_ok'], 'inv2'] = None\n",
    "\n",
    "#Ok, we continue\n",
    "data.eval('''\n",
    "    dp_used1 = kdef1.shift(1) * a1_depk_exip_bea\n",
    "    dp_used2 = 0\n",
    "    dp_used3 = kdef3.shift(1) * a1_depk_ip_bea\n",
    "    dp_used4 = kdef4.shift(1) * a1_depk_all_bea\n",
    "    dp_used5 = ppent.shift(1) * a1_depk_all_bea\n",
    "    dp_used6 = 0\n",
    "    \n",
    "    inv5 = capx  + inv2\n",
    "    kdef5 = ppent + k_int\n",
    "    \n",
    "    inv6 = at.diff()\n",
    "    kdef6 = at\n",
    "''', inplace = True)\n",
    "\n",
    "# Fix the vars\n",
    "data.loc[data['shift_not_ok'], [f'dp_used{i}' for i in range(1,7)] + ['inv6']] = None\n",
    "\n",
    "# Drop some stuff\n",
    "data = data.loc[:,~data.columns.str.contains('a1_depk')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually compute those variables\n",
    "for i in range(1,7):\n",
    "    data.eval(f'''\n",
    "        ik{i} = inv{i}/kdef{i}.shift(1)\n",
    "        nik{i} = (inv{i}-dp_used{i})/kdef{i}.shift(1)\n",
    "        ios{i} = inv{i}/os\n",
    "        nios{i} = (inv{i}-dp_used{i})/os\n",
    "        niv{i} = (inv{i}-dp_used{i})/mv.shift(1)        \n",
    "    ''', inplace = True)\n",
    "    \n",
    "    # Fix lags\n",
    "    data.loc[data['shift_not_ok'], [f'ik{i}', f'nik{i}', f'nios{i}', f'niv{i}']] = None\n",
    "    \n",
    "# Drop some vars\n",
    "data.drop(columns = ['ik2', 'ik5', 'ik6', 'ios5', 'ios6'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bbkinghome/tadej/miniconda3/envs/cf/lib/python3.7/site-packages/pandas/core/algorithms.py:1819: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n",
      "/bbkinghome/tadej/miniconda3/envs/cf/lib/python3.7/site-packages/pandas/core/algorithms.py:1819: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n",
      "/bbkinghome/tadej/miniconda3/envs/cf/lib/python3.7/site-packages/pandas/core/algorithms.py:1819: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n"
     ]
    }
   ],
   "source": [
    "# More variables\n",
    "data.eval('''\n",
    "    logat = log(at)\n",
    "    nblev = (bliab - che)/at\n",
    "    txtoi = txt/oiadp\n",
    "    txdba = txdb/at\n",
    "    cf = ib + dp\n",
    "    logsale = log(sale)\n",
    "    dlogsale = logsale.diff()\n",
    "    xrdat = xrd/at\n",
    "    xrdsale = xrd/sale\n",
    "\n",
    "    osk = os/ppent.shift()\n",
    "    osat = os/at.shift()\n",
    "\n",
    "    cfat = cf/at.shift()\n",
    "    cfk = cf/ppent.shift()\n",
    "\n",
    "    logemp = log(emp)\n",
    "    logq = log(q)\n",
    "    logq2 = log(q2)\n",
    "    logppe = log(ppent)\n",
    "    dlogemp = logemp.diff()\n",
    "    dlogppe = logppe.diff()\n",
    "\n",
    "    kemp = ppent/emp\n",
    "    kemp_PT = k_pt/emp\n",
    "''', inplace=True)\n",
    "\n",
    "# Fix some shifts\n",
    "fix_vars = ['dlogsale', 'osk', 'osat', 'cfat', 'cfk', 'dlogemp', 'dlogppe']\n",
    "data.loc[data['shift_not_ok'], fix_vars] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the firm continuing - did it appear before 1995 and kept appearing in 2010\n",
    "cnt_firm = lambda x: (x.min() <= 1995) & (x.max() >= 2010)\n",
    "data['continuing'] = data.groupby('gvkey')['year'].transform(cnt_firm)\n",
    "\n",
    "# Is the firm in manufacturing\n",
    "data['manufacturing'] = data['naics3'].between(310, 340)\n",
    "\n",
    "# Top X for at by year and indcode\n",
    "data['rank'] = data.groupby(['year', 'indcode'])['at'].apply(lambda x: x.rank(ascending = False))\n",
    "data['top5'] = data['rank'] <= 5\n",
    "data['top3'] = data['rank'] <= 3\n",
    "\n",
    "# Top X for me by year and indcode\n",
    "data['rank'] = data.groupby(['year', 'indcode'])['me'].apply(lambda x: x.rank(ascending = False))\n",
    "data['top5'] = data['rank'] <= 5\n",
    "data['top3'] = data['rank'] <= 3\n",
    "\n",
    "data.drop(columns = ['rank'], inplace = True)\n",
    "\n",
    "# Divestitures\n",
    "data['doind'] = (data['do'] != 0).fillna(False)\n",
    "data['sppeind'] = (data['sppe'] > 0).fillna(False)\n",
    "data['sivind'] = (data['siv'] > 0).fillna(False)\n",
    "data['aqcind'] = (data['aqc'] > 0).fillna(False)\n",
    "\n",
    "data.eval('''\n",
    "    sppek = sppe/ppent.shift(1)\n",
    "    aldoat = aldo/at.shift(1)\n",
    "    sivat = siv/at.shift(1)\n",
    "    aqcat = aqc/at.shift(1)\n",
    "    ltd = dltt + dd1\n",
    "''', inplace = True)\n",
    "\n",
    "# Fix lags\n",
    "data.loc[data['shift_not_ok'], ['aldoat', 'sivat', 'aqcat', 'sppek']] = None\n",
    "\n",
    "data['dd1d'] = (data['dd1']/data['ltd']).clip(0,1)\n",
    "data['dd3d'] = (data['dd3']/data['ltd']).clip(0,1)\n",
    "data['dd5d'] = (data['dd5']/data['ltd']).clip(0,1)\n",
    "\n",
    "# More row totals\n",
    "data['dd3c'] = rowtotal(data, ['dd1', 'dd3'], [1,-1])\n",
    "data['dd5c'] = rowtotal(data, ['dd1', 'dd5'], [1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bbkinghome/tadej/miniconda3/envs/cf/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Goodwill and attributes\n",
    "data.eval(''' \n",
    "    gwa = gdwl/at\n",
    "    intanexgw = intan - gdwl\n",
    "    intanexgwat = intanexgw/at\n",
    "    intanat = intan/at\n",
    "\n",
    "    pifoadj = pifo.fillna(0)\n",
    "    pifo_sh = pifo/pi\n",
    "    pifoadj_sh = pifoadj/pi\n",
    "    pifoind = pifo.notna()\n",
    "''', inplace = True)\n",
    "\n",
    "# Winsorize\n",
    "cols = data.columns[data.columns.str.contains('^(ik|nik|niv|ios|nios)')]\n",
    "\n",
    "for col in cols:\n",
    "    data[col] = data.groupby('year')[col].transform(winsor)\n",
    "    \n",
    "cols = ['osk', 'payos', 'cfat', 'pifo_sh', 'pifoadj_sh', 'intanat', 'intanexgwat', 'xrdat']\n",
    "for col in cols:\n",
    "    data[col] = data.groupby('year')[col].transform(winsor, l = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of dependence on external finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_rz13 = data['fopt'] + data['ap'].diff() - data['invt'].diff() - data['rect'].diff()\n",
    "data['fopt2'] = rowtotal(data, ['ibc', 'dpc', 'txdc', 'esubc', 'sppiv', 'fopo'])\n",
    "cf_rz7 = data['fopt2'] + data['ap'].diff() - data['invt'].diff() - data['rect'].diff()\n",
    "\n",
    "cf_rz13.loc[data['shift_not_ok']] = None\n",
    "cf_rz7.loc[data['shift_not_ok']] = None\n",
    "\n",
    "data['cf_rz'] = np.select([data['scf'].between(1, 3), data['scf'] == 7],\n",
    "                          [cf_rz13, cf_rz7], np.nan)\n",
    "\n",
    "# 10 year lagged total, with some conditions\n",
    "# First check if all of neqiss, ndebtiss, and capx present\n",
    "data_temp = data.loc[:, ['ndebtiss', 'neqiss', 'capx', 'cf_rz']]\n",
    "vars_not_ok = data_temp.isna().any(axis = 1)\n",
    "data_temp.loc[vars_not_ok, :] = 0 \n",
    "gby = data_temp.groupby(data['gvkey'])\n",
    "\n",
    "# Do the rolling sums - maybe min_periods should be added (set to 1?)\n",
    "data['cum_ndebtiss'] = gby['ndebtiss'].rolling(10).sum().droplevel(0)\n",
    "data['cum_neqiss'] = gby['neqiss'].rolling(10).sum().droplevel(0)\n",
    "data['cum_capx'] = gby['capx'].rolling(10).sum().droplevel(0)\n",
    "data['cum_cf_rz'] = gby['cf_rz'].rolling(10).sum().droplevel(0)\n",
    "\n",
    "# Some vars\n",
    "data.eval('''\n",
    "    extfindep_rz = (cum_capx - cum_cf_rz) / cum_capx\n",
    "    exteqfindep_rz = cum_neqiss / cum_capx\n",
    "    extdebtfindep_rz = cum_ndebtiss / cum_capx\n",
    "''', inplace = True)\n",
    "\n",
    "# Winsorizing\n",
    "data['extfindep_rz'] = data.groupby('year')['extfindep_rz'].transform(winsor, l = 0.01)\n",
    "data['exteqfindep_rz'] = data.groupby('year')['exteqfindep_rz'].transform(winsor, l = 0.01)\n",
    "data['extdebtfindep_rz'] = data.groupby('year')['extdebtfindep_rz'].transform(winsor, l = 0.01)\n",
    "\n",
    "# Bank dependence\n",
    "data['bankdep'] = data['sprating'].isna() & (data['ltd'] > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of volatility\n",
    "\n",
    "Here I compute the volatility of the sales (for some reason, actually of the log difference of sales), and of stock returns. \n",
    "\n",
    "Stock returns are obtained from WRDS, specifically from `compd.secm` database. I take `gvkey, trt1m (AS ret), datadate` variables, filtering to those where `ret` is not NA, `prclm` is higher than zero and `iid` is \"01\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std deviation for 5 or 10 period rolling window od dlogsale\n",
    "gby = data.groupby('gvkey')['dlogsale']\n",
    "data['sig_g5'] = gby.rolling(5, center = True).std(ddof = 0).droplevel(0)\n",
    "data['sig_g10'] = gby.rolling(10, center = True).std(ddof = 0).droplevel(0)\n",
    "\n",
    "# Get stock returns\n",
    "data_ret = db.raw_sql('''\n",
    "    SELECT gvkey, trt1m AS ret, datadate\n",
    "    FROM compd.secm \n",
    "    WHERE trt1m IS NOT NULL AND prclm > 0 AND iid = '01' \n",
    "''')\n",
    "data_ret['datadate'] = pd.to_datetime(data_ret['datadate'])\n",
    "\n",
    "# Create gvkey - indcode dict\n",
    "gvkey_indcode_dict = data[['gvkey', 'indcode']].drop_duplicates()\n",
    "\n",
    "# Merge dict to returns data\n",
    "data_ret = data_ret.merge(gvkey_indcode_dict, how = 'left')\n",
    "data_ret = data_ret.sort_values(['gvkey', 'datadate'])\n",
    "\n",
    "# Stdev and 6 month MA of stdev, by indcode\n",
    "data_ret = data_ret.pivot_table(index = ['indcode', 'datadate'], values = ['ret'], \n",
    "                                aggfunc = np.std).reset_index()\\\n",
    "                   .sort_values(['indcode', 'datadate'])\n",
    "\n",
    "data_ret['a1_stocksig'] = data_ret.groupby(['indcode'])['ret']\\\n",
    "                                  .rolling(6, center = True).mean().droplevel(0)\n",
    "\n",
    "# Get year, merge back to main\n",
    "data_ret['year'] = data_ret['datadate'].dt.year\n",
    "data_ret = data_ret.query('datadate.dt.month == 12')\n",
    "data = data.merge(data_ret[['year', 'indcode', 'a1_stocksig']], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry metrics for investment analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investment\n",
    "gby = data.groupby(['indcode', 'year'])\n",
    "gby_a = data.groupby('year')\n",
    "\n",
    "for i in range(1,7):\n",
    "    data[f'a1_inv{i}'] = gby[f'inv{i}'].transform('sum', min_count = 1)\n",
    "    data[f'a1_kdef{i}'] = gby[f'kdef{i}'].transform('sum', min_count = 1)\n",
    "    data[f'a1_dp{i}'] = gby[f'dp_used{i}'].transform('sum', min_count = 1)\n",
    "    \n",
    "    data[f'a_inv{i}'] = gby_a[f'inv{i}'].transform('sum', min_count = 1)\n",
    "    data[f'a_kdef{i}'] = gby_a[f'kdef{i}'].transform('sum', min_count = 1)\n",
    "    data[f'a_dp{i}'] = gby_a[f'dp_used{i}'].transform('sum', min_count = 1)\n",
    "    \n",
    "data['cfother'] = data['incf_def'] - rowtotal(data, ['ibc', 'xidoc', 'dpc', 'txdc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B/S metrics and use of proceeds\n",
    "data['temp'] = data['dvt'] + data['prstkc']\n",
    "\n",
    "gby = data.groupby(['indcode', 'year'])\n",
    "gby_a = data.groupby('year')\n",
    "\n",
    "vars_s = '''at emp bliab sale cogs xsga mv me k_int k_int_offbs ndebtiss\n",
    "            neqiss dv findef inv_def dnwc_def ibc xidoc dpc txdc cfother\n",
    "            che txt txdb oibdp cf gdwl cf_rz pifo pi capx ivch aqc sppe\n",
    "            siv ivstch ivaco xrd ivaeq ivao intan invt dd1 dd3c dd5c ltd incf_def'''\n",
    "vars_a = [x.strip() for x in vars_s.split(' ') if len(x) > 1]\n",
    "\n",
    "for var in vars_a:\n",
    "    data[f'a1_{var}'] = gby[var].transform('sum', min_count = 1)\n",
    "    data[f'a_{var}'] = gby_a[var].transform('sum', min_count = 1)\n",
    "\n",
    "data['a1_dp_cs'] = gby['dp'].transform('sum', min_count = 1)\n",
    "data['a_dp_cs'] = gby_a['dp'].transform('sum', min_count = 1)\n",
    "data['a1_os_cp'] = gby['os'].transform('sum', min_count = 1)\n",
    "data['a_os_cp'] = gby_a['os'].transform('sum', min_count = 1)\n",
    "data['a1_ppe'] = gby['ppent'].transform('sum', min_count = 1)\n",
    "data['a_ppe'] = gby_a['ppent'].transform('sum', min_count = 1)\n",
    "data['a1_pay'] = gby['temp'].transform('sum', min_count = 1)\n",
    "data['a_pay'] = gby_a['temp'].transform('sum', min_count = 1)\n",
    "data['a1_bb'] = gby['prstkc'].transform('sum', min_count = 1)\n",
    "data['a_bb'] = gby_a['prstkc'].transform('sum', min_count = 1)\n",
    "\n",
    "data['a1_logsale'] = np.log(data['a1_sale'])\n",
    "data['a1_logat'] = np.log(data['a1_at'])\n",
    "\n",
    "# k/emp + Peters & Taylor measures\n",
    "data.eval('''\n",
    "    a1_kemp = a1_ppe/a1_emp\n",
    "    a_kemp = a_ppe/a_emp\n",
    "\n",
    "    a1_shareintan_PT = a1_k_int/(a1_ppe+a1_k_int)\n",
    "    a1_share_int_offbs = a1_k_int_offbs /a1_k_int\n",
    "\n",
    "    a_shareintan_PT = a_k_int/(a_ppe+a_k_int)\n",
    "    a_share_int_offbs = a_k_int_offbs /a_k_int\n",
    "''', inplace = True)\n",
    "\n",
    "# CAPX + RD (same as ik4) and adjusted Q\n",
    "data['wt_pifo'] = 1 - data['pifoadj']/data['pi']\n",
    "data['temp'] = data['capx'] + data['xrd']\n",
    "data['temp1'] = data['mv'] * data['wt_pifo']\n",
    "data['temp2'] = data['at'] * data['wt_pifo']\n",
    "\n",
    "gby = data.groupby(['indcode', 'year'])\n",
    "gby_a = data.groupby('year')\n",
    "\n",
    "data['a_capxrd'] = gby_a['temp'].transform('sum', min_count = 1)\n",
    "\n",
    "data['a1_mvadj'] = gby_a['temp1'].transform('sum', min_count = 1)\n",
    "data['a1_atadj'] = gby_a['temp2'].transform('sum', min_count = 1)\n",
    "data['a_mvadj'] = gby_a['temp1'].transform('sum', min_count = 1)\n",
    "data['a_atadj'] = gby_a['temp2'].transform('sum', min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of firms\n",
    "data['a1sic_N'] = data.groupby(['siccode', 'year'])['gvkey'].transform('count')\n",
    "data['a1_N'] = data.groupby(['indcode', 'year'])['gvkey'].transform('count')\n",
    "data['a_N'] = data.groupby('year')['gvkey'].transform('count')\n",
    "\n",
    "data['a1sic_logN'] = np.log(data['a1sic_N'])\n",
    "data['a1_logN'] = np.log(data['a1_N'])\n",
    "data['a_logN'] = np.log(data['a_N'])\n",
    "\n",
    "# Get bools for entry and exit\n",
    "data['entry'] = 0\n",
    "data.loc[data.groupby('gvkey').head(1).index, 'entry'] = 1\n",
    "\n",
    "data['exit'] = 0\n",
    "data.loc[data.groupby('gvkey').tail(1).index, 'exit'] = 1\n",
    "\n",
    "data['exitMA'] = ((data['exit'] == 1) & (data['dlrsn'] == 1)).astype(int)\n",
    "\n",
    "# Some agg entry/exit vars \n",
    "gby_s = data.groupby(['siccode', 'year'])\n",
    "gby = data.groupby(['indcode', 'year'])\n",
    "gby_a = data.groupby('year')\n",
    "\n",
    "data['a_entry'] = gby_a['entry'].transform('sum', min_count = 1)\n",
    "data['a1_entry'] = gby['entry'].transform('sum', min_count = 1)\n",
    "data['a1sic_entry'] = gby_s['entry'].transform('sum', min_count = 1)\n",
    "\n",
    "data['a_exit'] = gby_a['exit'].transform('sum', min_count = 1)\n",
    "data['a1_exit'] = gby['exit'].transform('sum', min_count = 1)\n",
    "data['a1sic_exit'] = gby_s['exit'].transform('sum', min_count = 1)\n",
    "\n",
    "data['a_exitMA'] = gby_a['exitMA'].transform('sum', min_count = 1)\n",
    "data['a1_exitMA'] = gby['exitMA'].transform('sum', min_count = 1)\n",
    "data['a1sic_exitMA'] = gby_s['exitMA'].transform('sum', min_count = 1)\n",
    "\n",
    "data.eval('''\n",
    "    a_entryrate = a_entry/a_N\n",
    "    a_exitrate = a_exit/a_N\n",
    "    a_exitMArate = a_exitMA/a_N\n",
    "\n",
    "    a1_entryrate = a1_entry/a1_N\n",
    "    a1_exitrate = a1_exit/a1_N\n",
    "    a1_exitMArate = a1_exitMA/a1_N\n",
    "\n",
    "    a1sic_entryrate = a1_entry/a1sic_N\n",
    "    a1sic_exitrate = a1_exit/a1sic_N\n",
    "''', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorizing\n",
    "data['a1_entryrate'] = data.groupby('year')['a1_entryrate'].transform(winsor, l = 0.03)\n",
    "data['a1_exitrate'] = data.groupby('year')['a1_exitrate'].transform(winsor, l = 0.03)\n",
    "\n",
    "# Herfindal and lerner\n",
    "data.eval('''\n",
    "    ss1 = sale/a1_sale\n",
    "    ss1_mv = mv/a1_mv\n",
    "    ss1_cf = cf/a1_cf\n",
    "\n",
    "    li = (oibdp - dp) / sale\n",
    "    a1_li = (a1_oibdp - a1_dp_cs)/a1_sale\n",
    "    a_li = (a_oibdp - a_dp_cs)/a_sale\n",
    "''', inplace = True)\n",
    "\n",
    "gby = data.groupby(['indcode', 'year'])\n",
    "\n",
    "data['herf_s'] = gby['ss1'].transform(lambda x: (x*x).sum(min_count = 1))\n",
    "data['herf_mv'] = gby['ss1_mv'].transform(lambda x: (x*x).sum(min_count = 1))\n",
    "data['herf_cf'] = gby['ss1_cf'].transform(lambda x: (x*x).sum(min_count = 1))\n",
    "\n",
    "data['li'] = data.groupby('year')['li'].transform(winsor, l = 0.03)\n",
    "\n",
    "# Concentration\n",
    "data = data.sort_values(['indcode', 'year', 'sale'], ascending = False).reset_index(drop = True)\n",
    "gby = data.groupby(['indcode', 'year'])\n",
    "\n",
    "# Top X share in mv\n",
    "data['a1_cpcon1_sale'] = gby['sale'].transform(top_x_share, x = 1).min()\n",
    "data['a1_cpcon2_sale'] = gby['sale'].transform(top_x_share, x = 2).min()\n",
    "data['a1_cpcon4_sale'] = gby['sale'].transform(top_x_share, x = 4).min()\n",
    "data['a1_cpcon8_sale'] = gby['sale'].transform(top_x_share, x = 8).min()\n",
    "data['a1_cpcon20_sale'] = gby['sale'].transform(top_x_share, x = 20).min()\n",
    "data['a1_cpcon50_sale'] = gby['sale'].transform(top_x_share, x = 50).min()\n",
    "\n",
    "# Top X share in sales\n",
    "data['a1_cpcon4_mv'] = gby['mv'].transform(top_x_share, x = 4).min()\n",
    "data['a1_cpcon8_mv'] = gby['mv'].transform(top_x_share, x = 8).min()\n",
    "data['a1_cpcon20_mv'] = gby['mv'].transform(top_x_share, x = 20).min()\n",
    "data['a1_cpcon50_mv'] = gby['mv'].transform(top_x_share, x = 50).min()\n",
    "\n",
    "# Productivity\n",
    "data = data.sort_values(['gvkey', 'year']).reset_index(drop = True)\n",
    "data['shift_not_ok'] = data['gvkey'] != data['gvkey'].shift(1)\n",
    "\n",
    "data['roc'] = data['oibdp'] / data['ppent'].shift(1)\n",
    "data.loc[data['shift_not_ok'], 'roc'] = None\n",
    "data['a1sd_roc'] = data.groupby(['indcode', 'year'])['roc'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median and mean\n",
    "vars_s = '''blev q logq q2 logq2 ik1 ik3 ik4 nik1 nik2 nik3 nik4 nik5 nik6 ios1 ios2 ios3 ios4 nios1 nios2 nios3 nios4 nios5 nios6 \n",
    "            niv1 niv2 niv3 niv4 niv5 niv6 osk bba bbos paya payos gwa intanat intanexgwat dlogemp dlogppe logat cfat nblev txtoi dlogsale txdba \n",
    "            extfindep_rz exteqfindep_rz extdebtfindep_rz pifo_sh pifoadj_sh pifoind xrdat xrdsale sprating AAtoAAA BBBtoA bankdep age logage sig_g5 sig_g10\n",
    "            sppeind sppek sivind sivat aqcind aqcat kemp li q_tot k_int k_int_offbs shareintan'''\n",
    "vars_a = [x.strip() for x in vars_s.split(' ') if len(x) > 1]\n",
    "\n",
    "gby = data.groupby(['indcode', 'year'])\n",
    "gby_a = data.groupby('year')\n",
    "\n",
    "for var in vars_a:\n",
    "    data[f'a1m_{var}'] = gby[var].transform('mean')\n",
    "    data[f'a1med_{var}'] = gby[var].transform('median')\n",
    "\n",
    "    data[f'am_{var}'] = gby_a[var].transform('median')\n",
    "    data[f'amed_{var}'] = gby_a[var].transform('median')\n",
    "    \n",
    "for var in ['q', 'logq', 'ik1', 'nik1']:\n",
    "    data.loc[data['a1_N'] < 5, f'a1med_{var}'] = None\n",
    "    data.loc[data['a1_N'] < 5, f'a1m_{var}'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with Bushee\n",
    "data_bushee = pd.read_stata('Data/Intermediate/bushee_firmmap.dta')\n",
    "data = data.merge(data_bushee, how = 'left')\n",
    "\n",
    "# Firm counts\n",
    "data['populated'] = data.filter(like = 'pctsharetot').notna().any(axis = 1).astype(int)\n",
    "data['firmcount'] = data.groupby(['indcode', 'year'])['populated'].transform('sum')\n",
    " \n",
    "# Compute aggregate metrics for analyses: \n",
    "gby = data.groupby(['year', 'indcode'])\n",
    "gby_a = data.groupby('year')\n",
    "\n",
    "for type_inv in ['QIX', 'TRA', 'DED', 'NA']:\n",
    "    data[f'a1med_owntot{type_inv}'] = gby[f'pctsharetot{type_inv}'].transform('median')\n",
    "    data[f'a1m_owntot{type_inv}'] = gby[f'pctsharetot{type_inv}'].transform('mean')\n",
    "\n",
    "    data[f'a1med_ownins{type_inv}'] = gby[f'pctshareins{type_inv}'].transform('median')\n",
    "    data[f'a1m_ownins{type_inv}'] = gby[f'pctshareins{type_inv}'].transform('mean')\n",
    "\n",
    "    data[f'amed_owntot{type_inv}'] = gby_a[f'pctsharetot{type_inv}'].transform('median')\n",
    "    data[f'am_owntot{type_inv}'] = gby_a[f'pctsharetot{type_inv}'].transform('mean')\n",
    "\n",
    "    data[f'amed_ownins{type_inv}'] = gby_a[f'pctshareins{type_inv}'].transform('median')\n",
    "    data[f'am_ownins{type_inv}'] = gby_a[f'pctshareins{type_inv}'].transform('mean')\n",
    "    \n",
    "    wt_a = lambda x: pd.Series({\n",
    "        f'a_owntot{type_inv}': wt_mean(x[f'pctsharetot{type_inv}'], weights = x['me']),\n",
    "        f'a_ownins{type_inv}': wt_mean(x[f'pctshareins{type_inv}'], weights = x['me'])\n",
    "    })\n",
    "    wt_a1 = lambda x: pd.Series({\n",
    "        f'a1_owntot{type_inv}': wt_mean(x[f'pctsharetot{type_inv}'], weights = x['me']),\n",
    "        f'a1_owntot{type_inv}': wt_mean(x[f'pctshareins{type_inv}'], weights = x['me'])\n",
    "    })    \n",
    "\n",
    "    data = data.merge(gby_a.apply(wt_a), right_index = True, left_on = 'year')\n",
    "    data = data.merge(gby.apply(wt_a1), right_index = True, left_on = ['year', 'indcode'])\n",
    "    \n",
    "    # Set to null where there are less than 5 firms\n",
    "    v_new = [f'a1med_owntot{type_inv}', f'a1m_owntot{type_inv}', f'a1med_ownins{type_inv}',\n",
    "             f'a1m_ownins{type_inv}', f'amed_owntot{type_inv}', f'am_owntot{type_inv}',\n",
    "             f'amed_ownins{type_inv}', f'am_ownins{type_inv}', f'a_owntot{type_inv}',\n",
    "             f'a_ownins{type_inv}', f'a1_owntot{type_inv}', f'a1_owntot{type_inv}']\n",
    "    \n",
    "    data.loc[data['firmcount'] < 5, v_new] = None\n",
    "\n",
    "data.rename(columns = {\n",
    "    'pctshareinsTRA': 'owninsTRA',\n",
    "    'pctshareinsQIX': 'owninsQIX',\n",
    "    'pctshareinsDED': 'owninsDED',\n",
    "    'pctshareinsNA': 'owninsNA',\n",
    "    'pctsharetotTRA': 'owntotTRA',\n",
    "    'pctsharetotQIX': 'owntotQIX',\n",
    "    'pctsharetotDED': 'owntotDED',\n",
    "    'pctsharetotNA': 'owntotNA'    \n",
    "}, inplace = True)\n",
    "\n",
    "data['a1m_pctinsown'] = gby['pctinsown'].transform('mean')\n",
    "data['a1med_pctinsown'] = gby['pctinsown'].transform('median')\n",
    "\n",
    "data['am_pctinsown'] = gby_a['pctinsown'].transform('mean')\n",
    "data['amed_pctinsown'] = gby_a['pctinsown'].transform('median')\n",
    "\n",
    "data['a1_pctinsown'] = rowtotal(data, ['a1_owntotTRA', 'a1_owntotQIX', \n",
    "                                       'a1_owntotDED', 'a1_owntotNA'])\n",
    "data['a_pctinsown'] = rowtotal(data, ['a_owntotTRA', 'a_owntotQIX', \n",
    "                                       'a_owntotDED', 'a_owntotNA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final variables\n",
    "gby_s = data.groupby(['siccode', 'year'])\n",
    "\n",
    "data['a1sicm_q'] = gby_s['q'].transform('mean')\n",
    "data['a1sicm_osk'] = gby_s['osk'].transform('mean')\n",
    "data['a1sicm_logage'] = gby_s['logage'].transform('mean')\n",
    "data['a1sicm_logat'] = gby_s['logat'].transform('mean')\n",
    "\n",
    "data['a1sic_mv'] = gby_s['mv'].transform('sum', min_count = 1)\n",
    "data['a1sic_at'] = gby_s['at'].transform('sum', min_count = 1)\n",
    "data['a1sic_sale'] = gby_s['sale'].transform('sum', min_count = 1)\n",
    "\n",
    "data = data.sort_values(['gvkey', 'year']).reset_index(drop = True)\n",
    "data['shift_not_ok'] = data['gvkey'] != data['gvkey'].shift(1)\n",
    "\n",
    "data.eval('''\n",
    "    a1sic_logsale = log(a1sic_sale)\n",
    "    a1sic_logat = log(a1sic_at)\n",
    "\n",
    "    logmv = log(mv)\n",
    "    logme = log(me)\n",
    "    dlogmv = logmv.diff()\n",
    "''', inplace = True)\n",
    "\n",
    "data.loc[data['shift_not_ok'],'dlogmv'] = None\n",
    "\n",
    "gby = data.groupby(['indcode', 'year'])\n",
    "\n",
    "data['a1sic_logsale'] = np.log(data['a1sic_sale'])\n",
    "data['a1sic_logat'] = np.log(data['a1sic_at'])\n",
    "data['a1mad_logmv'] = gby['dlogmv'].transform('mad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bbkinghome/tadej/miniconda3/envs/cf/lib/python3.7/site-packages/pandas/io/stata.py:2136: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    b'do'   ->   _do\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  warnings.warn(ws, InvalidColumnName)\n"
     ]
    }
   ],
   "source": [
    "# Dividing produced some np.infs, replace with NA\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "\n",
    "data.to_stata('Data/Intermediate/data_firm.dta', write_index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
