{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ff_category import FFCategory\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Initialize connection\n",
    "db = wrds.Connection(wrds_username='tadej')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compustat\n",
    "\n",
    "Here I get the required compustat data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Annual fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download\n",
    "\n",
    "I get the following rows from `funda` table in the `compd` database:\n",
    "- `gvkey, fyear, fic, sich, csho, prcc_f, at, lt, pstk, dltt`\n",
    "- `dlc, act, ppegt, dvt, prstkc, oiadp, txt, xint, che, ppent, capx, gdwl`\n",
    "- `naicsh`\n",
    "\n",
    "The only filtering I do is to throw out things without a year (happens sometimes), and apply the standard, criptic data selection that prevents duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL select string\n",
    "select_str = '''\n",
    "SELECT gvkey, fyear, fic, sich, csho, prcc_f, at, lt, pstk, dltt, dlc, \n",
    "       act, ppegt, dvt, prstkc, oiadp, txt, xint, che, ppent, capx, gdwl,\n",
    "       naicsh\n",
    "FROM compd.funda \n",
    "WHERE indfmt='INDL' AND datafmt='STD' AND popsrc='D' AND consol='C'\n",
    "        AND fyear IS NOT NULL\n",
    "'''\n",
    "\n",
    "# Get data\n",
    "data_fa = db.raw_sql(select_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New variables\n",
    "\n",
    "Here I create the following new variables:\n",
    "- `me`: Market value of equity, obtained by multiplying common shares outsdanding with their price \n",
    "- `be`: Book value of equity, equal to total assets minus liabilities minus equity (some measure of it)\n",
    "- `bliab`: Book value of liabilities: assets minus equity\n",
    "- `mv`: Market value: Market value of equity + assets - book equity\n",
    "- `blev`: Leverage: book value of libilities divided by assets\n",
    "- `q`: Tobin's q (market value of the firm divided by assets)\n",
    "- `q2`: Alternative Tobin's q\n",
    "- `paya`: Payouts over assets\n",
    "- `bba`: Buybacks over assets\n",
    "- `os`: Net operating surplus\n",
    "- `payos`: Payouts over operating surplus\n",
    "- `bbos`: Buybacks over operating surplus\n",
    "- `ca`: Cash holdings over assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fa.eval('''\n",
    "    me = csho*prcc_f \n",
    "    be = at - lt - pstk \n",
    "    bliab = at - be\n",
    "    blev = bliab/at\n",
    "    mv = me + at - be\n",
    "    q = mv/at\n",
    "    q2 = (me+dltt+dlc-act)/ppegt\n",
    "    paya = (dvt + prstkc)/ at\n",
    "    bba = prstkc / at\n",
    "    os = oiadp - txt - xint  \n",
    "    payos = (dvt + prstkc)/ os\n",
    "    bbos = prstkc/ os\n",
    "    ca = che / at''', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peters and Taylor Q\n",
    "\n",
    "Here I download the Peters and Taylor (improved) Q measure, that is the `total_q` table in the `totalq` database.\n",
    "\n",
    "I download the variables `gvkey`, `fyear`, `q_tot`, `k_int` and `k_int_offbs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL select string\n",
    "select_str = '''\n",
    "SELECT fyear, gvkey, q_tot, k_int, k_int_offbs\n",
    "FROM totalq.total_q\n",
    "'''\n",
    "\n",
    "# Get data\n",
    "data_q = db.raw_sql(select_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratings data\n",
    "\n",
    "I download the data from Compustat, specifically, I use the Ratings table under North America daily. I then apply a recoding of the ratings, as shown below.\n",
    "\n",
    "**Warning**: That dataset is obsolete, it has not been updated since 2017. There is a newer credit ratings dataset (under Capital Q), however, based on my investigation, it has much less data than the old one (and some data it has is ambiguous), so it is not a suitable substitute here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_str = '''\n",
    "SELECT date_part('year', datadate) AS year, splticrm, gvkey\n",
    "FROM compd.adsprate\n",
    "WHERE date_part('month', datadate) = 12\n",
    "AND splticrm IS NOT NULL\n",
    "AND splticrm NOT IN ('N.M.', 'Suspended', 'SD')\n",
    "'''\n",
    "\n",
    "data_r = db.raw_sql(select_str)\n",
    "\n",
    "# Numerify the ratings\n",
    "replace_dict = {\n",
    "    'AAA': 1, 'AA': 2, 'A': 3, 'BBB': 4, 'BB': 5, 'B': 6,\n",
    "    'CCC': 7, 'CC': 8, 'C': 9, 'D': 10\n",
    "}\n",
    "\n",
    "data_r['sprating'] = data_r['splticrm']\n",
    "\n",
    "# The order is important here\n",
    "for key, value in replace_dict.items():\n",
    "    data_r.loc[data_r.sprating.str.contains(key), 'sprating'] = str(value)\n",
    "    \n",
    "data_r['sprating'] = data_r['sprating'].astype(int) \n",
    "\n",
    "# Create some rating booleans\n",
    "data_r['AAtoAAA'] = data_r.sprating <= 2\n",
    "data_r['BBBtoA'] = (data_r.sprating <= 4) & (data_r.sprating >= 3)\n",
    "\n",
    "# Drop some variables\n",
    "data_r.drop(columns = ['splticrm'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and minor calculations\n",
    "\n",
    "Here I merge the three datasets downloaded so far, and compute some new variables:\n",
    "- `k_pt`: A measure of capital, basically adding physical and intangible capital together\n",
    "- `shareintant`: Share of intangible capital in the above measure\n",
    "\n",
    "After that, I sort data by `gvkey` and `year` and add cummulative count for `gvkey` : this will be the age of the firm (assuming it appears in the dataset every year of its existence). I then log this to produce `logage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge fundamentals annual and Q data\n",
    "data = data_fa.merge(data_q, on = ['gvkey', 'fyear'], how = 'left')\n",
    "\n",
    "# Rename fyear to year and then merge\n",
    "data.rename(columns = {'fyear': 'year'}, inplace = True)\n",
    "data = data.merge(data_r, on = ['gvkey', 'year'], how = 'left')\n",
    "\n",
    "# New variables\n",
    "data.eval('''\n",
    "    k_pt = ppent + k_int\n",
    "    shareintan = k_int / k_pt''', inplace = True)\n",
    "\n",
    "# Sort and age\n",
    "data.sort_values(['gvkey', 'year'], inplace = True)\n",
    "data['logage'] = np.log(data.groupby('gvkey').cumcount() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregates pre-filtering\n",
    "\n",
    "Here I compute some aggregates (by year), which are later used to produce a graph. These aggregates are:\n",
    "- `a_capx_all_preEx`: sum of `capx`\n",
    "- `a_capx_US_preEx`: sum of `capx`, only for US firms\n",
    "- `a_pay_preEx`: sum of `dvt` and `prstkc`\n",
    "- `a_prstkc_preEx`: sum of `prstkc`\n",
    "- `a_at_preEx`: sum of `at`\n",
    "- `a_paya_preEx = a_pay_preEx / a_at_preEx`\n",
    "- `a_bba_preEx = a_prstkc_preEx / a_at_preEx`\n",
    "\n",
    "**Note**: Check the thing about goodwill later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute aggregate variables from data\n",
    "data_agg = data.groupby('year').apply(lambda x: pd.Series({\n",
    "    'a_capx_all_preEx': x['capx'].sum(),\n",
    "    'a_capx_US_preEx': x.query('fic == \"USA\"')['capx'].sum(),\n",
    "    'a_pay_preEx': x.eval('dvt + prstkc').sum(),\n",
    "    'a_prstkc_preEx': x['prstkc'].sum(),\n",
    "    'a_at_preEx': x['at'].sum()\n",
    "}))\n",
    "\n",
    "# Add some composite aggregate variables\n",
    "data_agg.eval('''\n",
    "    a_paya_preEx = a_pay_preEx / a_at_preEx\n",
    "    a_bba_preEx = a_prstkc_preEx / a_at_preEx''', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and aggregates\n",
    "\n",
    "Finally, I do some filtering:\n",
    "1. Drop all entries where `at`, `gvkey`, `be`, `me`, `bliab` or `q` are NA, as well as those where `at` is smaller than 1 (million), and where either `be` or `me` are negative.\n",
    "2. Drop all entries with years smaller or equal to 1961, and drop a peculiar case with `gvkey` 4828 and `year` 2001.\n",
    "3. Drop all entries with `sich` in \\[4900, 4999\\] (utilities), \\[6000, 6999\\] (financials) or \\[5300, 5399\\] (real estate)\n",
    "4. Keep only US companies (`fic = USA`)\n",
    "\n",
    "At this point I also compute some aggregates (I do this right after step 3).\n",
    "\n",
    "Finally, I assign the Fama-French categories to the SIC numbers (based on my own implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs, filtering\n",
    "data.dropna(subset = ['year', 'gvkey', 'be', 'me', 'bliab', 'q'], inplace = True)\n",
    "data.query('at >= 1 & be > 0 & me > 0 & year > 1961 & ~(gvkey == 4828 & year == 2001)', inplace = True)\n",
    "\n",
    "# Exclude some sic codes\n",
    "data.query('~sich.between(4900, 4999) & ~sich.between(6000, 6999) & ~sich.between(5300, 5399)', inplace = True)\n",
    "\n",
    "# Some additional aggregates\n",
    "k = data.groupby('year').apply(lambda x: pd.Series({\n",
    "    'a_capx_all_wEx': x['capx'].sum(),\n",
    "    'a_capx_US_wEx': x.query('fic == \"USA\"')['capx'].sum(),\n",
    "}))\n",
    "\n",
    "data_agg[['a_capx_all_wEx', 'a_capx_US_wEx']] = k\n",
    "\n",
    "# Drop non-USA\n",
    "data.query('fic == \"USA\"', inplace = True)\n",
    "\n",
    "# Get the 10 category FF classification\n",
    "ff = FFCategory('https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/Siccodes10.zip')\n",
    "data['ff10'] = ff.assign_ff(data.sich)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation mapping\n",
    "\n",
    "Here I create some segmentation variables (mostly NAICS related)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAICS 3\n",
    "\n",
    "I create a new variable `naics3` from the first 3 digits of `naicsh`. Where this is missing, I do the following assignment:\n",
    "- For each sic and naics pair I count the number of entries appearing in it.\n",
    "- For each sic, I take the highest NAICS (from the previous step)\n",
    "- When possible, I match this naics to corresponding SIC\n",
    "\n",
    "Additionally, I also manually adjust some naics values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naics to string\n",
    "data['naics_str'] = data.naicsh.astype(str).str.slice(0, -2)\n",
    "data.loc[data.naicsh.isna(), 'naics_str'] = np.nan\n",
    "\n",
    "# Default NAICS3\n",
    "data['naics3'] = data.loc[data.naics_str.str.len() >= 3, 'naics_str'].str.slice(0,3)\n",
    "data['naics3'] = data['naics3'].astype(float)\n",
    "\n",
    "# Create the matching dictionary\n",
    "agg = data.groupby(['sich', 'naics3']).size()\n",
    "match_dict = agg.groupby(level = 0, group_keys = False).nlargest(1)\n",
    "match_dict = match_dict.reset_index().drop(columns= [0])\n",
    "\n",
    "# Reset index before merging\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Match\n",
    "naics3_matched = data[['sich']].merge(match_dict, on = 'sich', how = 'left')['naics3']\n",
    "data.loc[data.naics3.isna(), 'naics3'] = naics3_matched\n",
    "\n",
    "# Special matching\n",
    "data.loc[data.naics3.isna() & (data.sich == 3412), 'naics3'] = 332"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEA Codes\n",
    "\n",
    "Here I map to BEA codes and segments, using the provided excel and stata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bea_codes = pd.read_excel()\n",
    "bea_segments = pd.read_stata()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
